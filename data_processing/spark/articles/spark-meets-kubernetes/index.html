<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-data_processing docs-doc-id-spark/articles/spark-meets-kubernetes">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Spark meets Kubernetes: the complete guide | Sharek.dev</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://helkaroui.github.io/data_processing/spark/articles/spark-meets-kubernetes"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="tech, big data, spark, GitOps"><meta data-rh="true" name="author" content="Hamza EL KAROUI"><meta data-rh="true" name="twitter:site" content="@stringly_typed"><meta data-rh="true" name="twitter:creator" content="@stringly_typed"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-data_processing-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-data_processing-current"><meta data-rh="true" property="og:title" content="Spark meets Kubernetes: the complete guide | Sharek.dev"><meta data-rh="true" name="description" content="In today&#x27;s data-driven world, the ability to efficiently process and analyze large datasets is crucial. Apache Spark has been a go-to solution for big data processing, while Kubernetes has emerged as a leading platform for container orchestration. Together, these two technologies form a potent combination, offering a scalable and flexible environment for managing and executing Spark workloads."><meta data-rh="true" property="og:description" content="In today&#x27;s data-driven world, the ability to efficiently process and analyze large datasets is crucial. Apache Spark has been a go-to solution for big data processing, while Kubernetes has emerged as a leading platform for container orchestration. Together, these two technologies form a potent combination, offering a scalable and flexible environment for managing and executing Spark workloads."><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://helkaroui.github.io/data_processing/spark/articles/spark-meets-kubernetes"><link data-rh="true" rel="alternate" href="https://helkaroui.github.io/data_processing/spark/articles/spark-meets-kubernetes" hreflang="en"><link data-rh="true" rel="alternate" href="https://helkaroui.github.io/data_processing/spark/articles/spark-meets-kubernetes" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Sharek.dev RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Sharek.dev Atom Feed"><link rel="stylesheet" href="/assets/css/styles.d3cf9590.css">
<link rel="preload" href="/assets/js/runtime~main.0814bc8f.js" as="script">
<link rel="preload" href="/assets/js/main.481c47ca.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Sharek" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="Sharek" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Sharek.dev</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Data Processing</a><ul class="dropdown__menu"><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/data_processing/spark">Apache Spark</a></li><li><a class="dropdown__link" href="/data_processing/kafka/">Apache Kafka</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Containers</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/containers/docker/quick_start">Docker</a></li><li><a class="dropdown__link" href="/containers/kubernetes/quick_start">Kubernetes</a></li><li><a class="dropdown__link" href="/containers/podman/quick_start">Podman</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Developer&#x27;s Corner</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/developer/languages/scala/quick_start">Scala</a></li><li><a class="dropdown__link" href="/developer/languages/python/quick_start">Python</a></li><li><a class="dropdown__link" href="/developer/languages/go/quick_start">Go</a></li><li><a class="dropdown__link" href="/developer/languages/rust/quick_start">Rust</a></li><li><a class="dropdown__link" href="/developer/languages/java/quick_start">Java</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Home Lab</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/home_lab">My Home Lab</a></li><li><a class="dropdown__link" href="/home_lab/media_center/funkwhale">FunkWhale</a></li></ul></div></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Projects</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/projects/scheduler">Scheduler</a></li></ul></div><a class="navbar__item navbar__link" href="/about">About</a><a href="https://github.com/helkaroui" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/data_processing/spark/">Apache Spark</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/data_processing/spark/">Quick Start</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/data_processing/spark/tutorials/getting-started-with-spark">Tutorials</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/data_processing/spark/optimizations/quick_wins">Optimizations</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/data_processing/spark/internals/index">Spark Internals</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/data_processing/spark/customization/create-plugin">Customizations</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/data_processing/spark/articles/databricks-spark-developer-associate-exam">Other articles</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/data_processing/spark/articles/databricks-spark-developer-associate-exam">Databricks Certified Associate Developer for Apache Spark Exam questions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/data_processing/spark/articles/spark-meets-go">Spark meets Go Lang</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/data_processing/spark/articles/spark-meets-kubernetes">Spark meets Kubernetes: the complete guide</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/data_processing/spark/articles/whats-new-in-spark3">Exploring the Exciting New Features in Apache Spark 3.0</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data_processing/kafka/">Kafka</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/data_processing/flink/">Flink</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Apache Spark</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Other articles</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Spark meets Kubernetes: the complete guide</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Spark meets Kubernetes: the complete guide</h1></header><p><img loading="lazy" src="/assets/images/spark-on-k8s-41426f75a2dc0b543430a8d6c2fdcc15.png" width="1650" height="1060" class="img_ev3q"></p><p>In today&#x27;s data-driven world, the ability to efficiently process and analyze large datasets is crucial. Apache Spark has been a go-to solution for big data processing, while Kubernetes has emerged as a leading platform for container orchestration. Together, these two technologies form a potent combination, offering a scalable and flexible environment for managing and executing Spark workloads.</p><p>In this context, we want to share our take aways from migrating old Spark Standalone clusters to kubernetes using Spark Submit.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="project-context">Project context<a href="#project-context" class="hash-link" aria-label="Direct link to Project context" title="Direct link to Project context">​</a></h2><p>The client&#x27;s infrastructure is build on top of OpenStack, thus most of services are installed manually or using Ansible, on top of virtual machines. With these constraints, the big data team has build multiple Spark Standalone clusters for each of their environments.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="migration-goals--constraints">Migration Goals &amp; Constraints<a href="#migration-goals--constraints" class="hash-link" aria-label="Direct link to Migration Goals &amp; Constraints" title="Direct link to Migration Goals &amp; Constraints">​</a></h2><p><strong>Constraints</strong> </p><p>The client is very demanding regarding security and authorization, thus the architects forbidden different teams from installing k8s operators.</p><p><strong>Goals</strong> </p><p>The integration of Apache Spark with Kubernetes has opened up new horizons for running Spark workloads in a more efficient and flexible manner. Here&#x27;s why this combination is so compelling:</p><p><strong>1. Resource Management and Isolation</strong></p><p>Kubernetes provides fine-grained control over resources, enabling Spark applications to be isolated in containers with specific CPU and memory limits. This ensures that Spark jobs don&#x27;t contend for resources with other applications running on the same cluster.</p><p><strong>2. Scalability</strong></p><p>Kubernetes makes it easy to scale Spark clusters up or down based on workload demands. This dynamic scaling ensures optimal resource utilization, reducing infrastructure costs.</p><p><strong>3. Multi-Tenancy</strong></p><p>Kubernetes supports multi-tenancy, allowing different teams or users to share the same cluster securely. Each Spark application can run within its own namespace, ensuring data and resource isolation.</p><p><strong>4. Portability</strong></p><p>With Kubernetes, you can deploy Spark applications consistently across various environments, from on-premises data centers to public clouds. This portability simplifies deployment and minimizes compatibility issues.</p><p><strong>5. Simplified Operations</strong></p><p>Kubernetes abstracts away many of the complexities associated with managing Spark clusters. It automates tasks like scaling, monitoring, and recovery, reducing the operational overhead.</p><p><strong>6. Efficient Resource Utilization</strong></p><p>Kubernetes&#x27; bin-packing capabilities ensure efficient use of cluster resources. Spark pods are scheduled on worker nodes with available resources, minimizing wastage.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="getting-started-with-spark-on-kubernetes">Getting Started with Spark on Kubernetes<a href="#getting-started-with-spark-on-kubernetes" class="hash-link" aria-label="Direct link to Getting Started with Spark on Kubernetes" title="Direct link to Getting Started with Spark on Kubernetes">​</a></h2><p>TBD</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="architecture">Architecture<a href="#architecture" class="hash-link" aria-label="Direct link to Architecture" title="Direct link to Architecture">​</a></h3><p>TBD</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-spark-works-on-kubernetes">How Spark Works on Kubernetes<a href="#how-spark-works-on-kubernetes" class="hash-link" aria-label="Direct link to How Spark Works on Kubernetes" title="Direct link to How Spark Works on Kubernetes">​</a></h3><p>TBD</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="spark-submit-vs-spark-operator">Spark-Submit vs Spark Operator<a href="#spark-submit-vs-spark-operator" class="hash-link" aria-label="Direct link to Spark-Submit vs Spark Operator" title="Direct link to Spark-Submit vs Spark Operator">​</a></h3><p><code>spark-submit</code> and the Spark Operator are two different approaches for running Apache Spark applications on Kubernetes. Each has its own advantages and use cases, and the choice between them depends on your specific requirements and infrastructure setup.</p><ul><li><p>Use spark-submit when:</p><ul><li>You need maximum flexibility and control over Spark configurations.</li><li>You are already comfortable with the spark-submit command.</li><li>Your Spark applications need to run in various cluster environments.</li></ul></li><li><p>Use Spark Operator when:</p><ul><li>You want a Kubernetes-native, automated solution.</li><li>You prefer to define and manage Spark applications as Kubernetes resources.</li><li>You need dynamic scaling and resource management features.</li><li>You are using Kubernetes extensively in your infrastructure.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dynamic-scaling">Dynamic Scaling<a href="#dynamic-scaling" class="hash-link" aria-label="Direct link to Dynamic Scaling" title="Direct link to Dynamic Scaling">​</a></h3><p>Dynamic scaling refers to the ability to automatically adjust the number of Spark executors in response to workload demands. This feature allows applications to efficiently utilize cluster resources while maintaining optimal performance.</p><ol><li><p><strong>Initial Deployment</strong>:
When you submit a Spark application to run on Kubernetes, you define an initial number of executor pods based on your workload requirements and resource availability. These executor pods run alongside the Spark driver pod.</p></li><li><p><strong>Monitoring Metrics</strong>:
Kubernetes, along with monitoring tools like Prometheus and Grafana, collects metrics about the Spark application&#x27;s resource usage, such as CPU and memory consumption, as well as the progress of tasks within the application.</p></li><li><p><strong>Resource Utilization Thresholds</strong>:
You can configure resource utilization thresholds or policies that define when the cluster should scale up or down based on predefined criteria. These thresholds are often defined in terms of CPU and memory utilization.</p></li><li><p><strong>Scaling Trigger</strong>:
When the metrics collected breach the predefined thresholds, Kubernetes triggers the scaling process. If resource utilization is consistently high and exceeds the defined threshold, Kubernetes initiates the scaling up process to allocate more resources to the Spark application.</p></li><li><p><strong>Scaling Up</strong>:</p><ul><li>Kubernetes increases the desired number of Spark executor pods by creating new pods.</li><li>These new executor pods join the existing Spark driver pod and executor pods to distribute the workload.</li><li>The Spark application can take advantage of the additional resources to process data faster.</li></ul></li><li><p><strong>Continued Monitoring</strong>:
Kubernetes and monitoring tools continue to monitor the Spark application&#x27;s resource usage. If resource utilization drops below a certain threshold or the workload decreases, Kubernetes may trigger a scaling down process to reduce the number of executor pods.</p></li><li><p><strong>Scaling Down</strong>:</p><ul><li>Kubernetes gracefully terminates the selected executor pods.</li><li>Spark gracefully handles the termination of these executor pods, ensuring that in-progress tasks are not lost and that data is not corrupted.</li><li>Once the executor pods have been safely terminated, the Spark application continues to run with the remaining resources.</li></ul></li><li><p><strong>Iterative Process</strong>:
Dynamic scaling is an iterative process that can occur multiple times during the execution of a Spark application. It allows the application to adapt to changing resource demands, ensuring efficient resource utilization without manual intervention.</p></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="getting-hands-dirty">Getting hands dirty<a href="#getting-hands-dirty" class="hash-link" aria-label="Direct link to Getting hands dirty" title="Direct link to Getting hands dirty">​</a></h2><p>TBD</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="requirements">Requirements<a href="#requirements" class="hash-link" aria-label="Direct link to Requirements" title="Direct link to Requirements">​</a></h3><ul><li>Kubernetes cluster / Minikube</li><li>Docker</li><li>Skaffold</li><li>Kustomize</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="setting-up-a-the-project">Setting up a the project<a href="#setting-up-a-the-project" class="hash-link" aria-label="Direct link to Setting up a the project" title="Direct link to Setting up a the project">​</a></h3><p><a href="https://skaffold.dev/" target="_blank" rel="noopener noreferrer"><strong>Skaffold</strong></a> is a command line tool that facilitates continuous development for container based &amp; Kubernetes applications. </p><p><img loading="lazy" src="/assets/images/skaffold-architecture-04ec12ff6f7ea3c159df0acd2f8b7cda.png" width="8305" height="4309" class="img_ev3q"></p><p><a href="https://kustomize.io/" target="_blank" rel="noopener noreferrer"><strong>Kustomize</strong></a> is a Kubernetes configuration transformation tool that allows you to customize untemplated YAML files, leaving the original files intact.</p><p><a href="https://minikube.sigs.k8s.io/docs/start/" target="_blank" rel="noopener noreferrer"><strong>Minikube</strong></a> is local Kubernetes, focusing on making it easy to learn and develop for Kubernetes.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="1--the-project-structure">1- The project structure<a href="#1--the-project-structure" class="hash-link" aria-label="Direct link to 1- The project structure" title="Direct link to 1- The project structure">​</a></h4><p>We will start by creating a project with a structure that emphasis the separation of rules, i.e. we seperate the code base, the service component and the environment specifics on which the application will run. This concept is also called <code>Environment-Agnostic Design</code> also known as <code>environment-agnostic architecture</code> or <code>platform-agnostic design</code>.</p><p>In that sperit, here is our project structure :</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── images</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   ├── base-images</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   │   └── spark-base-image</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   └── custom-images</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│       └── spark-app-example</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── services</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   ├── sparkhs</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   ├── spark-job-example</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   └── spark-reverse-proxy</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">└── deployment</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ├── base</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    └── overlays</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        ├── dev</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        └── prod</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li><p><strong>images :</strong> The docker images folder is where lives our code base, it can also be split into two folders :</p><ul><li><em>base-images</em> for base docker that will be used to build other images. Example : spark, jdk, python, sbt, gradle.</li><li><em>custom-images</em> which can enhirit from base images, and holds images with our code base.</li></ul></li><li><p><strong>services :</strong> Here we define the services that will run our docker images on kubernetes.</p></li><li><p><strong>Deployment :</strong> this folder holds resources and variants of environment configurations - like <code>development</code>, <code>staging</code> and <code>production</code> - using overlays that modify a common base.</p></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="2--spark-base-image">2- Spark Base Image<a href="#2--spark-base-image" class="hash-link" aria-label="Direct link to 2- Spark Base Image" title="Direct link to 2- Spark Base Image">​</a></h4><p>We decided to create a custom spark docker image rather than using the provided docker image, in order to showcase the possibility of customizing Spark upon the project needs.</p><p>The easiest way is to mimic the Dockerfile located in Spark <a href="https://github.com/apache/spark/blob/master/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile" target="_blank" rel="noopener noreferrer">Repository</a></p><p>We reduce the dockerfile to it&#x27;s minimum and we add a stage to build spark from source using maven.</p><div class="language-Dockerfile codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-Dockerfile codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">FROM maven:3.9.4-eclipse-temurin AS BUILD</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">TBD</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">FROM eclipse-temurin:17-jre as RUNTIME</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ARG spark_uid=185</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">RUN set -ex &amp;&amp; \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    apt-get update &amp;&amp; \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ln -s /lib /lib64 &amp;&amp; \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    apt install -y bash tini &amp;&amp; \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    mkdir -p /opt/spark &amp;&amp; \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    mkdir -p /opt/spark/examples &amp;&amp; \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    mkdir -p /opt/spark/work-dir &amp;&amp; \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    touch /opt/spark/RELEASE &amp;&amp; \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    rm /bin/sh &amp;&amp; \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ln -sv /bin/bash /bin/sh &amp;&amp; \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    echo &quot;auth required pam_wheel.so use_uid&quot; &gt;&gt; /etc/pam.d/su &amp;&amp; \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    chgrp root /etc/passwd &amp;&amp; chmod ug+rw /etc/passwd &amp;&amp; \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    rm -rf /var/cache/apt/* &amp;&amp; rm -rf /var/lib/apt/lists/*</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">COPY entrypoint.sh /opt/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">COPY decom.sh /opt/</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ENV SPARK_HOME /opt/spark</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">WORKDIR /opt/spark/work-dir</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">RUN chmod g+w /opt/spark/work-dir \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      &amp;&amp; chmod a+x /opt/decom.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ENTRYPOINT [ &quot;/opt/entrypoint.sh&quot; ]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># Specify the User that the actual main process will run as</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">USER ${spark_uid}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="3--spark-app-example-image">3- Spark App Example Image<a href="#3--spark-app-example-image" class="hash-link" aria-label="Direct link to 3- Spark App Example Image" title="Direct link to 3- Spark App Example Image">​</a></h4><p>To showcase a fully working spark application, we create a basic Scala/Spark application with two scripts :</p><ul><li><em>Compute Pi</em> this script will compute an approximation to PI and log the result. The code is also found within <a href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala" target="_blank" rel="noopener noreferrer">Spark-Examples</a> module.</li><li><em>Streaming Example</em> A basic streaming script with Spark structured streaming.</li></ul><p>We first start by creating an SBT project as follow :</p><p>1- <code>cd</code> to the folder <code>images/custom-images/</code>.</p><p>2- Run the following command <code>sbt new scala/scala3.g8</code>. This pulls the ‘scala3’ template from GitHub. It will also create a target folder, which you can ignore.</p><p>3- When prompted, name the application <code>spark-app-example</code>. This will create a project called “spark-app-example”.</p><p>4- Let’s take a look at what just got generated:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── build.sbt</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── project</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">│   └── build.properties</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">├── README.md</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">└── src</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ├── main</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    │   └── scala</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    │       └── Main.scala</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    └── test</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        └── scala</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            └── MySuite.scala</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>5- Adding a dependency in the <code>build.sbt</code> file :</p><div class="language-sbt codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sbt codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">libraryDependencies ++= Seq(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  (&quot;org.apache.spark&quot; %% &quot;spark-sql&quot; % &quot;3.5.0&quot; % &quot;provided&quot;).cross(CrossVersion.for3Use2_13)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>Because there is no Scala 3 version of spark-sql available, we use CrossVersion for3Use2_13 to tell sbt we want the Scala 2.13 version of this library</p></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="minimal-working-example">Minimal working example<a href="#minimal-working-example" class="hash-link" aria-label="Direct link to Minimal working example" title="Direct link to Minimal working example">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="building--deploying-example">Building &amp; Deploying Example<a href="#building--deploying-example" class="hash-link" aria-label="Direct link to Building &amp; Deploying Example" title="Direct link to Building &amp; Deploying Example">​</a></h4><h3 class="anchor anchorWithStickyNavbar_LWe7" id="kubernetes-components">Kubernetes Components<a href="#kubernetes-components" class="hash-link" aria-label="Direct link to Kubernetes Components" title="Direct link to Kubernetes Components">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="spark-submit-image">Spark Submit Image<a href="#spark-submit-image" class="hash-link" aria-label="Direct link to Spark Submit Image" title="Direct link to Spark Submit Image">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="spark-submit-service">Spark Submit Service<a href="#spark-submit-service" class="hash-link" aria-label="Direct link to Spark Submit Service" title="Direct link to Spark Submit Service">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="using-pod-template">Using Pod Template<a href="#using-pod-template" class="hash-link" aria-label="Direct link to Using Pod Template" title="Direct link to Using Pod Template">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="adding-a-configmap">Adding a configmap<a href="#adding-a-configmap" class="hash-link" aria-label="Direct link to Adding a configmap" title="Direct link to Adding a configmap">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="ingress">Ingress<a href="#ingress" class="hash-link" aria-label="Direct link to Ingress" title="Direct link to Ingress">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="spark-history-server">Spark History Server<a href="#spark-history-server" class="hash-link" aria-label="Direct link to Spark History Server" title="Direct link to Spark History Server">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="ui-proxy">UI Proxy<a href="#ui-proxy" class="hash-link" aria-label="Direct link to UI Proxy" title="Direct link to UI Proxy">​</a></h4><h3 class="anchor anchorWithStickyNavbar_LWe7" id="deploying-the-spark-pi-demo-application">Deploying the Spark Pi Demo Application<a href="#deploying-the-spark-pi-demo-application" class="hash-link" aria-label="Direct link to Deploying the Spark Pi Demo Application" title="Direct link to Deploying the Spark Pi Demo Application">​</a></h3><p>TBD</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="checking-the-logs">Checking the logs<a href="#checking-the-logs" class="hash-link" aria-label="Direct link to Checking the logs" title="Direct link to Checking the logs">​</a></h4><p>TBD</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="accessing-the-spark-ui">Accessing the Spark UI<a href="#accessing-the-spark-ui" class="hash-link" aria-label="Direct link to Accessing the Spark UI" title="Direct link to Accessing the Spark UI">​</a></h4><p>TBD</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="spark-job-execution-history">Spark Job Execution History<a href="#spark-job-execution-history" class="hash-link" aria-label="Direct link to Spark Job Execution History" title="Direct link to Spark Job Execution History">​</a></h4><p>TBD</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="monitoring">Monitoring<a href="#monitoring" class="hash-link" aria-label="Direct link to Monitoring" title="Direct link to Monitoring">​</a></h4><p>TBD</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="continuous-development">Continuous Development<a href="#continuous-development" class="hash-link" aria-label="Direct link to Continuous Development" title="Direct link to Continuous Development">​</a></h3><p>TBD </p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="hot-reloading">Hot reloading<a href="#hot-reloading" class="hash-link" aria-label="Direct link to Hot reloading" title="Direct link to Hot reloading">​</a></h4><p>TBD</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="pros-and-cons-of-spark-submit-with-k8s">Pros and Cons of Spark Submit with K8s<a href="#pros-and-cons-of-spark-submit-with-k8s" class="hash-link" aria-label="Direct link to Pros and Cons of Spark Submit with K8s" title="Direct link to Pros and Cons of Spark Submit with K8s">​</a></h2><p>TBD</p><p><strong>Pros of Spark with K8s:</strong></p><ul><li>...</li><li>...</li></ul><p><strong>Cons of Spark with K8s:</strong></p><ul><li>...</li><li>...</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><p>TBD</p><p><strong>Resources:</strong></p><ul><li><a href="https://skaffold.dev/" target="_blank" rel="noopener noreferrer">https://skaffold.dev/</a></li><li><a href="https://blog.cellenza.com/en/data/using-spark-with-kubernetes-k8s/" target="_blank" rel="noopener noreferrer">https://blog.cellenza.com/en/data/using-spark-with-kubernetes-k8s/</a></li><li><a href="https://devopscube.com/kustomize-tutorial/" target="_blank" rel="noopener noreferrer">https://devopscube.com/kustomize-tutorial/</a></li><li><a href="https://xebia.com/blog/using-scala-3-with-spark/" target="_blank" rel="noopener noreferrer">Using Scala 3 with Spark</a></li></ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/data_processing/spark/articles/spark-meets-go"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Spark meets Go Lang</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/data_processing/spark/articles/whats-new-in-spark3"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Exploring the Exciting New Features in Apache Spark 3.0</div></a></nav><div style="margin-top:3em"></div></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#project-context" class="table-of-contents__link toc-highlight">Project context</a></li><li><a href="#migration-goals--constraints" class="table-of-contents__link toc-highlight">Migration Goals &amp; Constraints</a></li><li><a href="#getting-started-with-spark-on-kubernetes" class="table-of-contents__link toc-highlight">Getting Started with Spark on Kubernetes</a><ul><li><a href="#architecture" class="table-of-contents__link toc-highlight">Architecture</a></li><li><a href="#how-spark-works-on-kubernetes" class="table-of-contents__link toc-highlight">How Spark Works on Kubernetes</a></li><li><a href="#spark-submit-vs-spark-operator" class="table-of-contents__link toc-highlight">Spark-Submit vs Spark Operator</a></li><li><a href="#dynamic-scaling" class="table-of-contents__link toc-highlight">Dynamic Scaling</a></li></ul></li><li><a href="#getting-hands-dirty" class="table-of-contents__link toc-highlight">Getting hands dirty</a><ul><li><a href="#requirements" class="table-of-contents__link toc-highlight">Requirements</a></li><li><a href="#setting-up-a-the-project" class="table-of-contents__link toc-highlight">Setting up a the project</a></li><li><a href="#kubernetes-components" class="table-of-contents__link toc-highlight">Kubernetes Components</a></li><li><a href="#deploying-the-spark-pi-demo-application" class="table-of-contents__link toc-highlight">Deploying the Spark Pi Demo Application</a></li><li><a href="#continuous-development" class="table-of-contents__link toc-highlight">Continuous Development</a></li></ul></li><li><a href="#pros-and-cons-of-spark-submit-with-k8s" class="table-of-contents__link toc-highlight">Pros and Cons of Spark Submit with K8s</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/helkaroui" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Sharek.dev, Inc.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.0814bc8f.js"></script>
<script src="/assets/js/main.481c47ca.js"></script>
</body>
</html>