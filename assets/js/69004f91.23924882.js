"use strict";(self.webpackChunksharek_dev=self.webpackChunksharek_dev||[]).push([[4650],{5082:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>g,contentTitle:()=>m,default:()=>w,frontMatter:()=>d,metadata:()=>h,toc:()=>b});var r=a(7462),n=a(7294),l=a(3905),i=a(5471);function s(e){const{children:t,title:a="Collapse"}=e;return n.createElement(i.Z,null,n.createElement("summary",{mdxType:"summary"},a),t)}var o=a(4866),p=a(5162),u=a(614);function c(e){const{url:t,language:a,title:r=""}=e,[l,i]=(0,n.useState)("");return(0,n.useEffect)((()=>{fetch(t).then((e=>e.text())).then((e=>{i(e)}))}),[]),n.createElement("div",null,n.createElement(u.Z,{language:a,title:r,showLineNumbers:!0},l))}function k(e){const{url:t,language:a,title:r=""}=e;return n.createElement(i.Z,null,n.createElement("summary",{mdxType:"summary"},r),n.createElement(c,{url:t,language:a}))}const d={title:"Spark meets Kubernetes: the complete guide",toc_min_heading_level:2,toc_max_heading_level:5},m=void 0,h={unversionedId:"spark/articles/spark-meets-kubernetes",id:"spark/articles/spark-meets-kubernetes",title:"Spark meets Kubernetes: the complete guide",description:"This article is still in review \ud83d\udea7 by the co-authors.",source:"@site/sections/data_processing/spark/06-articles/spark-meets-kubernetes.md",sourceDirName:"spark/06-articles",slug:"/spark/articles/spark-meets-kubernetes",permalink:"/data_processing/spark/articles/spark-meets-kubernetes",draft:!1,tags:[],version:"current",frontMatter:{title:"Spark meets Kubernetes: the complete guide",toc_min_heading_level:2,toc_max_heading_level:5},sidebar:"docs",previous:{title:"Spark meets Go Lang",permalink:"/data_processing/spark/articles/spark-meets-go"},next:{title:"Exploring the Exciting New Features in Apache Spark 3.0",permalink:"/data_processing/spark/articles/whats-new-in-spark3"}},g={},b=[{value:"Project context",id:"project-context",level:2},{value:"Goals &amp; Constraints",id:"goals--constraints",level:3},{value:"Architecture",id:"architecture",level:3},{value:"Spark-Submit Architecture",id:"spark-submit-architecture",level:4},{value:"Project Architecture",id:"project-architecture",level:4},{value:"Getting Started",id:"getting-started",level:2},{value:"Requirements",id:"requirements",level:3},{value:"Container &amp; Infrastructure",id:"container--infrastructure",level:4},{value:"Dev tools",id:"dev-tools",level:4},{value:"The project structure",id:"the-project-structure",level:3},{value:"Building docker images",id:"building-docker-images",level:3},{value:"Spark Base Image",id:"spark-base-image",level:4},{value:"Spark App Example Image",id:"spark-app-example-image",level:4},{value:"1st approach: Kubernetes Job",id:"1st-approach-kubernetes-job",level:3},{value:"S3 Service",id:"s3-service",level:4},{value:"Spark Submit Service",id:"spark-submit-service",level:4},{value:"Deploying the Spark Pi Demo Application",id:"deploying-the-spark-pi-demo-application",level:4},{value:"Checking the logs",id:"checking-the-logs",level:4},{value:"Accessing the Spark UI",id:"accessing-the-spark-ui",level:4},{value:"Adding the Reverse Proxy",id:"adding-the-reverse-proxy",level:4},{value:"Adding Spark History Server",id:"adding-spark-history-server",level:4},{value:"2nd approach: Airflow Scheduler",id:"2nd-approach-airflow-scheduler",level:3},{value:"Creating an Airflow Service",id:"creating-an-airflow-service",level:4},{value:"Deploying the Spark Pi Demo Application",id:"deploying-the-spark-pi-demo-application-1",level:4},{value:"Checking the logs",id:"checking-the-logs-1",level:4},{value:"Conclusion",id:"conclusion",level:2}],f={toc:b},v="wrapper";function w(e){let{components:t,...n}=e;return(0,l.kt)(v,(0,r.Z)({},f,n,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("admonition",{type:"warning"},(0,l.kt)("p",{parentName:"admonition"},"This article is still in review \ud83d\udea7 by the co-authors.")),(0,l.kt)("p",null,(0,l.kt)("img",{src:a(9266).Z,width:"1650",height:"1060"})),(0,l.kt)("p",null,"In today's data-driven world, the ability to efficiently process and analyze large datasets is crucial. Apache Spark has been a go-to solution for big data processing, while Kubernetes has emerged as a leading platform for container orchestration. Together, these two technologies form a potent combination, offering a scalable and flexible environment for managing and executing Spark workloads."),(0,l.kt)("p",null,"In this context, we want to share our take aways from migrating old Spark Standalone clusters to kubernetes using Spark Submit."),(0,l.kt)("h2",{id:"project-context"},"Project context"),(0,l.kt)("p",null,"The client's infrastructure is build on top of OpenStack, thus most of services are installed manually or using Ansible, on top of virtual machines. With these constraints, the big data team has build multiple Spark Standalone clusters for each of their environments."),(0,l.kt)("h3",{id:"goals--constraints"},"Goals & Constraints"),(0,l.kt)("admonition",{type:"info"},(0,l.kt)("p",{parentName:"admonition"},(0,l.kt)("strong",{parentName:"p"},"Dynamic Scaling"),"\nDynamic scaling refers to the ability to automatically adjust the number of Spark executors in response to workload demands. This feature allows applications to efficiently utilize cluster resources while maintaining optimal performance."),(0,l.kt)(s,{title:"Learn more ...",mdxType:"Collapse"},(0,l.kt)("ol",{parentName:"admonition"},(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"Initial Deployment"),":\nWhen you submit a Spark application to run on Kubernetes, you define an initial number of executor pods based on your workload requirements and resource availability. These executor pods run alongside the Spark driver pod.")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"Monitoring Metrics"),":\nKubernetes, along with monitoring tools like Prometheus and Grafana, collects metrics about the Spark application's resource usage, such as CPU and memory consumption, as well as the progress of tasks within the application.")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"Resource Utilization Thresholds"),":\nYou can configure resource utilization thresholds or policies that define when the cluster should scale up or down based on predefined criteria. These thresholds are often defined in terms of CPU and memory utilization.")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"Scaling Trigger"),":\nWhen the metrics collected breach the predefined thresholds, Kubernetes triggers the scaling process. If resource utilization is consistently high and exceeds the defined threshold, Kubernetes initiates the scaling up process to allocate more resources to the Spark application.")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"Scaling Up"),":"))),(0,l.kt)("ul",{parentName:"admonition"},(0,l.kt)("li",{parentName:"ul"},"Kubernetes increases the desired number of Spark executor pods by creating new pods."),(0,l.kt)("li",{parentName:"ul"},"These new executor pods join the existing Spark driver pod and executor pods to distribute the workload."),(0,l.kt)("li",{parentName:"ul"},"The Spark application can take advantage of the additional resources to process data faster.")),(0,l.kt)("ol",{parentName:"admonition",start:6},(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"Continued Monitoring"),":\nKubernetes and monitoring tools continue to monitor the Spark application's resource usage. If resource utilization drops below a certain threshold or the workload decreases, Kubernetes may trigger a scaling down process to reduce the number of executor pods.")),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"Scaling Down"),":"))),(0,l.kt)("ul",{parentName:"admonition"},(0,l.kt)("li",{parentName:"ul"},"Kubernetes gracefully terminates the selected executor pods."),(0,l.kt)("li",{parentName:"ul"},"Spark gracefully handles the termination of these executor pods, ensuring that in-progress tasks are not lost and that data is not corrupted."),(0,l.kt)("li",{parentName:"ul"},"Once the executor pods have been safely terminated, the Spark application continues to run with the remaining resources.")),(0,l.kt)("ol",{parentName:"admonition",start:8},(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("strong",{parentName:"li"},"Iterative Process"),":\nDynamic scaling is an iterative process that can occur multiple times during the execution of a Spark application. It allows the application to adapt to changing resource demands, ensuring efficient resource utilization without manual intervention.")))),(0,l.kt)("h3",{id:"architecture"},"Architecture"),(0,l.kt)("h4",{id:"spark-submit-architecture"},"Spark-Submit Architecture"),(0,l.kt)("p",null,"Spark Submit can be used to submit a Spark Application directly to a Kubernetes cluster. The flow would be as follows:"),(0,l.kt)("p",null,(0,l.kt)("img",{src:a(2670).Z,width:"1866",height:"862"})),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"The client, that lives outside or inside the kubernetes cluster, submit an application and asks the k8s API to create the driver"),(0,l.kt)("li",{parentName:"ol"},"k8s creates Spark driver running within a Kubernetes pod."),(0,l.kt)("li",{parentName:"ol"},"The driver requests k8s to create executors which are also running within Kubernetes pods."),(0,l.kt)("li",{parentName:"ol"},"k8s creates the executors "),(0,l.kt)("li",{parentName:"ol"},"The driver is notified and connects to them."),(0,l.kt)("li",{parentName:"ol"},"The driver executes application code on executors.")),(0,l.kt)("p",null,"When the application completes, the executor pods terminate and are cleaned up, but the driver pod persists logs and remains in \u201ccompleted\u201d state in the Kubernetes API until it\u2019s eventually garbage collected or manually cleaned up."),(0,l.kt)("admonition",{type:"note"},(0,l.kt)("p",{parentName:"admonition"},"Note that in the completed state, the driver pod does not use any computational or memory resources.")),(0,l.kt)("admonition",{type:"info"},(0,l.kt)("p",{parentName:"admonition"},"There is another approche to schedule spark application on kubernetes, which by using a ",(0,l.kt)("inlineCode",{parentName:"p"},"Spark Operator"),". The operator should be installed into kubernetes cluster."),(0,l.kt)("p",{parentName:"admonition"},"Each has its own advantages and use cases, and the choice between them depends on your specific requirements and infrastructure setup."),(0,l.kt)("ul",{parentName:"admonition"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Use spark-submit when:"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"You need maximum flexibility and control over Spark configurations."),(0,l.kt)("li",{parentName:"ul"},"You are already comfortable with the spark-submit command."),(0,l.kt)("li",{parentName:"ul"},"Your Spark applications need to run in various cluster environments."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Use Spark Operator when:"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},"You want a Kubernetes-native, automated solution."),(0,l.kt)("li",{parentName:"ul"},"You prefer to define and manage Spark applications as Kubernetes resources."),(0,l.kt)("li",{parentName:"ul"},"You need dynamic scaling and resource management features."),(0,l.kt)("li",{parentName:"ul"},"You are using Kubernetes extensively in your infrastructure."))))),(0,l.kt)("h4",{id:"project-architecture"},"Project Architecture"),(0,l.kt)("p",null,"In this project, we will showcase two ways to schedule Spark applications on kubernetes:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Native Kubernetes solution: using kubernetes Job resource type to submit the application."),(0,l.kt)("li",{parentName:"ul"},"Scheduler: using Airflow to schedule application.")),(0,l.kt)("p",null,"For these two solutions we propose the following architectures :"),(0,l.kt)(o.Z,{mdxType:"Tabs"},(0,l.kt)(p.Z,{value:"apple",label:"Using Kubernetes jobs",default:!0,mdxType:"TabItem"},(0,l.kt)("div",{style:{padding:"20px","background-color":"#f6f8fa"}},(0,l.kt)("p",null,(0,l.kt)("em",{parentName:"p"},"What is a kubernetes Job ?")),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. "),(0,l.kt)("p",{parentName:"blockquote"},"\u2014 kubernetes.io")),(0,l.kt)("p",null,"For more details about jobs, check the ",(0,l.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/workloads/controllers/job/"},"kubernetes docs"),"."),(0,l.kt)("p",null,(0,l.kt)("img",{src:a(2647).Z,width:"2122",height:"1602"})))),(0,l.kt)(p.Z,{value:"orange",label:"Using a scheduler: Airflow",mdxType:"TabItem"},(0,l.kt)("div",{style:{padding:"20px","background-color":"#f6f8fa"}},(0,l.kt)("p",null,(0,l.kt)("em",{parentName:"p"},"What is a kubernetes Job ?")),(0,l.kt)("blockquote",null,(0,l.kt)("p",{parentName:"blockquote"},"Apache Airflow\u2122 is an open-source platform for developing, scheduling, and monitoring batch-oriented workflows. Airflow\u2019s extensible Python framework enables you to build workflows connecting with virtually any technology."),(0,l.kt)("p",{parentName:"blockquote"},"\u2014 airflow.apache.org")),(0,l.kt)("p",null,"For more details about Airflow, check the ",(0,l.kt)("a",{parentName:"p",href:"https://airflow.apache.org/"},"officiel website"),"."),(0,l.kt)("p",null,(0,l.kt)("img",{src:a(4746).Z,width:"2122",height:"1602"}))))),(0,l.kt)("h2",{id:"getting-started"},"Getting Started"),(0,l.kt)("p",null,"I recommand you to download the project source code from my Github Repo ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/helkaroui/spark-on-k8s"},"helkaroui/spark-on-k8s"),", and try to follow the following steps. It will help you to understand the different sections by stadying the code."),(0,l.kt)("h3",{id:"requirements"},"Requirements"),(0,l.kt)("p",null,"In this project we will need to setup the following tools :"),(0,l.kt)("h4",{id:"container--infrastructure"},"Container & Infrastructure"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://docs.docker.com/engine/install/"},(0,l.kt)("strong",{parentName:"a"},"Docker")))),(0,l.kt)(s,{title:"How to install Docker ?",mdxType:"Collapse"},(0,l.kt)("p",null,"Set up Docker's Apt repository:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'  sudo apt-get update\n  && sudo apt-get install ca-certificates curl gnupg\n  && sudo install -m 0755 -d /etc/apt/keyrings\n  && curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n  && sudo chmod a+r /etc/apt/keyrings/docker.gpg\n  && echo "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu\n    "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n  && sudo apt-get update\n')),(0,l.kt)("p",null,"To install the latest version, run:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n")),(0,l.kt)("p",null,"Now add your user to the Docker group:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"sudo usermod -aG docker $USER && newgrp docker\n")),(0,l.kt)("p",null,"Verify that the Docker Engine installation is successful by running the hello-world image."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"sudo docker run hello-world\n"))),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://minikube.sigs.k8s.io/docs/start/"},(0,l.kt)("strong",{parentName:"a"},"Minikube"))," is local Kubernetes, focusing on making it easy to learn and develop for Kubernetes.kube")),(0,l.kt)(s,{title:"How to install Minikube ?",mdxType:"Collapse"},(0,l.kt)("p",null,"To install the latest version, run:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube && rm minikube-linux-amd64\n")),(0,l.kt)("p",null,"Verify that the Minikube installation is successful by starting it:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"minikube start\n")),(0,l.kt)("p",null,"You can check the available node by running:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"minikube kubectl get nodes\n")),(0,l.kt)("p",null,"And you can access the Dashboard with this command, the Dashboard will be launched at ",(0,l.kt)("a",{parentName:"p",href:"http://localhost:41169"},"localhost:41169")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"minikube dashboard\n"))),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://kubernetes.io/fr/docs/tasks/tools/install-kubectl/"},(0,l.kt)("strong",{parentName:"a"},"kubectl"))," is a command line tool used to run commands against Kubernetes clusters.")),(0,l.kt)(s,{title:"How to install Kubectl ?",mdxType:"Collapse"},(0,l.kt)("p",null,"For kubectl, we can either install it, or create an alias to ",(0,l.kt)("inlineCode",{parentName:"p"},"minikube kubectl"),"."),(0,l.kt)("p",null,(0,l.kt)("em",{parentName:"p"},"Method 1"),": Install kubectl"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" \\\n&& sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl \\\n&& rm kubectl\n')),(0,l.kt)("p",null,(0,l.kt)("em",{parentName:"p"},"Method 2:")," For simplicity, we will add an alias to our .bashrc file and apply it:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"vim ~/.bashrc\n\n# Add this line at the end\nalias kubectl=minikube kubectl\n# Save and apply the file\n\nsource ~/.bashrc\n"))),(0,l.kt)("h4",{id:"dev-tools"},"Dev tools"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://kustomize.io/"},(0,l.kt)("strong",{parentName:"a"},"Kustomize"))," is a Kubernetes configuration transformation tool that allows you to customize untemplated YAML files, leaving the original files intact.")),(0,l.kt)(s,{title:"How to install Kustomize ?",mdxType:"Collapse"},(0,l.kt)("p",null,"To install :"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh"  | bash \\\n&& sudo install kustomize /usr/local/bin/ \\\n&& rm kustomize\n')),(0,l.kt)("p",null,"Verify setup:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kustomize version\n"))),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://skaffold.dev/"},(0,l.kt)("strong",{parentName:"a"},"Skaffold"))," is a command line tool that facilitates continuous development for container based & Kubernetes applications.")),(0,l.kt)(s,{title:"How to install Skaffold ?",mdxType:"Collapse"},(0,l.kt)("p",null,"To install :"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/v2.0.4/skaffold-linux-amd64 \\\n&& chmod +x skaffold \\\n&& sudo mv skaffold /usr/local/bin\n")),(0,l.kt)("p",null,"Verify setup:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"skaffold version\n"))),(0,l.kt)("admonition",{type:"note"},(0,l.kt)("p",{parentName:"admonition"},"Skaffold handles the workflow for building, pushing and deploying your application. It is similar to ",(0,l.kt)("inlineCode",{parentName:"p"},"Docker-compose")," for kubernetes.\nThis enables you to focus on iterating on your application locally while Skaffold continuously deploys to your local or remote Kubernetes cluster, local Docker environment or Cloud Run project."),(0,l.kt)(s,{title:"How it works in short ?",mdxType:"Collapse"},(0,l.kt)("p",{parentName:"admonition"},(0,l.kt)("img",{src:a(9850).Z,width:"8305",height:"4309"}))),(0,l.kt)("p",{parentName:"admonition"},"Follow this ",(0,l.kt)("a",{parentName:"p",href:"https://skaffold.dev/docs/quickstart/"},"officiel tutorial")," if you\u2019re using to Skaffold. It walks through running Skaffold on a small Kubernetes app built with Docker inside minikube and deployed with kubectl.")),(0,l.kt)("h3",{id:"the-project-structure"},"The project structure"),(0,l.kt)("p",null,"We will start by creating a project with a structure that emphasis the separation of rules, i.e. we seperate the code base, the service component and the environment specifics on which the application will run. This concept is also called ",(0,l.kt)("inlineCode",{parentName:"p"},"Environment-Agnostic Design")," also known as ",(0,l.kt)("inlineCode",{parentName:"p"},"environment-agnostic architecture")," or ",(0,l.kt)("inlineCode",{parentName:"p"},"platform-agnostic design"),"."),(0,l.kt)("p",null,"In that sperit, here is our project structure :"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"\u251c\u2500\u2500 images\n\u2502   \u251c\u2500\u2500 base-images\n\u2502   \u2502   \u2514\u2500\u2500 spark-base-image\n\u2502   \u2514\u2500\u2500 custom-images\n\u2502       \u2514\u2500\u2500 spark-app-example\n\u251c\u2500\u2500 services\n\u2502   \u251c\u2500\u2500 sparkhs\n\u2502   \u251c\u2500\u2500 spark-job-example\n\u2502   \u2514\u2500\u2500 spark-reverse-proxy\n\u2514\u2500\u2500 deployment\n    \u251c\u2500\u2500 base\n    \u2514\u2500\u2500 overlays\n        \u251c\u2500\u2500 dev\n        \u2514\u2500\u2500 prod\n")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"images :")," The docker images folder is where lives our code base, it can also be split into two folders :"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("em",{parentName:"li"},"base-images")," for base docker that will be used to build other images. Example : spark, jdk, python, sbt, gradle."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("em",{parentName:"li"},"custom-images")," which can enhirit from base images, and holds images with our code base."))),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"services :")," Here we define the services that will run our docker images on kubernetes.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},(0,l.kt)("strong",{parentName:"p"},"Deployment :")," this folder holds resources and variants of environment configurations - like ",(0,l.kt)("inlineCode",{parentName:"p"},"development"),", ",(0,l.kt)("inlineCode",{parentName:"p"},"staging")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"production")," - using overlays that modify a common base."))),(0,l.kt)("h3",{id:"building-docker-images"},"Building docker images"),(0,l.kt)("h4",{id:"spark-base-image"},"Spark Base Image"),(0,l.kt)("p",null,"We decided to create a custom spark docker image rather than using the provided docker image, in order to showcase the possibility of customizing Spark upon the project needs."),(0,l.kt)("p",null,"The easiest way is to mimic the Dockerfile located in Spark ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/apache/spark/blob/master/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile"},"Repository")),(0,l.kt)("p",null,"We reduce the dockerfile to it's minimum and we add a stage to build spark from source using maven."),(0,l.kt)(k,{title:"images/base-images/spark-base-image/Dockerfile",url:"https://raw.githubusercontent.com/helkaroui/spark-on-k8s/1-project-structure/images/base-images/spark-base-image/Dockerfile",language:"docker",mdxType:"CollapseGithubCodeBlock"}),(0,l.kt)("admonition",{type:"note"},(0,l.kt)("p",{parentName:"admonition"},"At this point, we can test everything by building the docker image ",(0,l.kt)("inlineCode",{parentName:"p"},"docker build .")," The build will take couple of minutes, since we are compiling the source code. "),(0,l.kt)("p",{parentName:"admonition"},"We can of course replace this section, and download the binaries from ",(0,l.kt)("a",{parentName:"p",href:"https://spark.apache.org"},"https://spark.apache.org"),", to reduce the build time. But the aim here is to be able to easily customize Spark, add libraries and adapt it to our needs.")),(0,l.kt)("h4",{id:"spark-app-example-image"},"Spark App Example Image"),(0,l.kt)("p",null,"To showcase a fully working spark application, we create a basic Scala/Spark application with two scripts :"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("em",{parentName:"li"},"Compute Pi")," this script will compute an approximation to PI and log the result. The code is also included in ",(0,l.kt)("a",{parentName:"li",href:"https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala"},"Spark-Examples")," module."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("em",{parentName:"li"},"Streaming Example")," A basic streaming script with Spark structured streaming.")),(0,l.kt)("p",null,"We first start by creating an SBT project as follow :"),(0,l.kt)("p",null,"1- ",(0,l.kt)("inlineCode",{parentName:"p"},"cd")," to the folder ",(0,l.kt)("inlineCode",{parentName:"p"},"images/custom-images/"),"."),(0,l.kt)("p",null,"2- Run the following command ",(0,l.kt)("inlineCode",{parentName:"p"},"sbt new scala/scala3.g8"),". This pulls the \u2018scala3\u2019 template from GitHub. It will also create a target folder, which you can ignore."),(0,l.kt)("p",null,"3- When prompted, name the application ",(0,l.kt)("inlineCode",{parentName:"p"},"spark-app-example"),". This will create a project called \u201cspark-app-example\u201d."),(0,l.kt)("p",null,"4- Let\u2019s take a look at what just got generated:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"\u251c\u2500\u2500 build.sbt\n\u251c\u2500\u2500 project\n\u2502\xa0\xa0 \u2514\u2500\u2500 build.properties\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 main\n    \u2502\xa0\xa0 \u2514\u2500\u2500 scala\n    \u2502\xa0\xa0     \u2514\u2500\u2500 Main.scala\n    \u2514\u2500\u2500 test\n        \u2514\u2500\u2500 scala\n            \u2514\u2500\u2500 MySuite.scala\n")),(0,l.kt)("p",null,"5- Adding a dependency in the ",(0,l.kt)("inlineCode",{parentName:"p"},"build.sbt")," file :"),(0,l.kt)(k,{title:"images/custom-images/spark-app-example/build.sbt",url:"https://raw.githubusercontent.com/helkaroui/spark-on-k8s/1-project-structure/images/custom-images/spark-app-example/build.sbt",language:"scala",mdxType:"CollapseGithubCodeBlock"}),(0,l.kt)("p",null,"6- Add ",(0,l.kt)("inlineCode",{parentName:"p"},"sbt-assembly")," plugin under ",(0,l.kt)("inlineCode",{parentName:"p"},"project/plugins.sbt")," :"),(0,l.kt)(k,{title:"images/custom-images/spark-app-example/project/plugins.sbt",url:"https://raw.githubusercontent.com/helkaroui/spark-on-k8s/1-project-structure/images/custom-images/spark-app-example/project/plugins.sbt",language:"scala",mdxType:"CollapseGithubCodeBlock"}),(0,l.kt)("p",null,"7- Create the scala script ",(0,l.kt)("inlineCode",{parentName:"p"},"SparkPi")," under package ",(0,l.kt)("inlineCode",{parentName:"p"},"dev.sharek.examples")," :"),(0,l.kt)(k,{title:"images/custom-images/spark-app-example/src/main/scala/dev/sharek/examples/SparkPi.scala",url:"https://raw.githubusercontent.com/helkaroui/spark-on-k8s/1-project-structure/images/custom-images/spark-app-example/src/main/scala/dev/sharek/examples/SparkPi.scala",language:"scala",mdxType:"CollapseGithubCodeBlock"}),(0,l.kt)("p",null,"8- Finaly, we add the Dockerfile that builds the sbt project into jar files:"),(0,l.kt)(k,{title:"images/custom-images/spark-app-example/Dockerfile",url:"https://raw.githubusercontent.com/helkaroui/spark-on-k8s/1-project-structure/images/custom-images/spark-app-example/Dockerfile",language:"docker",mdxType:"CollapseGithubCodeBlock"}),(0,l.kt)("admonition",{type:"note"},(0,l.kt)("p",{parentName:"admonition"},"At this point, we can test everything by building the docker image ",(0,l.kt)("inlineCode",{parentName:"p"},"docker build ."))),(0,l.kt)("h3",{id:"1st-approach-kubernetes-job"},"1st approach: Kubernetes Job"),(0,l.kt)("p",null,"To be able to mimic a real big data architecture, we want to have a distribute storage layer for the spark application to store it's Dataframes.\nWe choosed S3, as it is widely used in big data projects."),(0,l.kt)("h4",{id:"s3-service"},"S3 Service"),(0,l.kt)("p",null,"The docker file:"),(0,l.kt)(k,{title:"services/s3/Dockerfile",url:"https://raw.githubusercontent.com/helkaroui/spark-on-k8s/1-project-structure/services/s3/Dockerfile",language:"docker",mdxType:"CollapseGithubCodeBlock"}),(0,l.kt)("h4",{id:"spark-submit-service"},"Spark Submit Service"),(0,l.kt)(k,{title:"services/spark-app-example/Dockerfile",url:"https://raw.githubusercontent.com/helkaroui/spark-on-k8s/1-project-structure/services/spark-app-example/Dockerfile",language:"docker",mdxType:"CollapseGithubCodeBlock"}),(0,l.kt)(k,{title:"services/spark-app-example/entrypoint.sh",url:"https://raw.githubusercontent.com/helkaroui/spark-on-k8s/1-project-structure/services/spark-app-example/entrypoint.sh",language:"bash",mdxType:"CollapseGithubCodeBlock"}),(0,l.kt)(k,{title:"services/spark-app-example/job.yaml",url:"https://raw.githubusercontent.com/helkaroui/spark-on-k8s/1-project-structure/services/spark-app-example/job.yaml",language:"yaml",mdxType:"CollapseGithubCodeBlock"}),(0,l.kt)("h4",{id:"deploying-the-spark-pi-demo-application"},"Deploying the Spark Pi Demo Application"),(0,l.kt)("p",null,"starting Minikube:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"minikube start\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get nodes\n")),(0,l.kt)(k,{title:"Makefile",url:"https://raw.githubusercontent.com/helkaroui/spark-on-k8s/1-project-structure/Makefile",language:"makefile",mdxType:"CollapseGithubCodeBlock"}),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"make spark-submit\n")),(0,l.kt)("h4",{id:"checking-the-logs"},"Checking the logs"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get pods\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl logs driver-ccccccc-\n")),(0,l.kt)("h4",{id:"accessing-the-spark-ui"},"Accessing the Spark UI"),(0,l.kt)("p",null,"The UI associated with any application can be accessed locally using kubectl port-forward."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl port-forward <driver-pod-name> 4040:4040\n")),(0,l.kt)("p",null,"Then, the Spark driver UI can be accessed on ",(0,l.kt)("a",{parentName:"p",href:"http://localhost:4040"},"http://localhost:4040"),"."),(0,l.kt)("admonition",{type:"info"},(0,l.kt)("p",{parentName:"admonition"},"Checking logs ....")),(0,l.kt)("h4",{id:"adding-the-reverse-proxy"},"Adding the Reverse Proxy"),(0,l.kt)(k,{title:"services/spark-reverse-proxy/Dockerfile",url:"https://raw.githubusercontent.com/helkaroui/spark-on-k8s/1-project-structure/services/spark-reverse-proxy/Dockerfile",language:"docker",mdxType:"CollapseGithubCodeBlock"}),(0,l.kt)("h4",{id:"adding-spark-history-server"},"Adding Spark History Server"),(0,l.kt)("h3",{id:"2nd-approach-airflow-scheduler"},"2nd approach: Airflow Scheduler"),(0,l.kt)("h4",{id:"creating-an-airflow-service"},"Creating an Airflow Service"),(0,l.kt)("h4",{id:"deploying-the-spark-pi-demo-application-1"},"Deploying the Spark Pi Demo Application"),(0,l.kt)("h4",{id:"checking-the-logs-1"},"Checking the logs"),(0,l.kt)("p",null,"TBD"),(0,l.kt)("h2",{id:"conclusion"},"Conclusion"),(0,l.kt)("p",null,"TBD"),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Resources:")),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://skaffold.dev/"},"https://skaffold.dev/")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://blog.cellenza.com/en/data/using-spark-with-kubernetes-k8s/"},"https://blog.cellenza.com/en/data/using-spark-with-kubernetes-k8s/")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://devopscube.com/kustomize-tutorial/"},"https://devopscube.com/kustomize-tutorial/")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://xebia.com/blog/using-scala-3-with-spark/"},"Using Scala 3 with Spark")),(0,l.kt)("li",{parentName:"ul"},"Install Minikube: ",(0,l.kt)("a",{parentName:"li",href:"https://minikube.sigs.k8s.io/docs/start/"},"https://minikube.sigs.k8s.io/docs/start/")),(0,l.kt)("li",{parentName:"ul"},"Install kubectl ",(0,l.kt)("a",{parentName:"li",href:"https://kubernetes.io/docs/tasks/tools/"},"https://kubernetes.io/docs/tasks/tools/")),(0,l.kt)("li",{parentName:"ul"},"Setup Airflow on k8s : ",(0,l.kt)("a",{parentName:"li",href:"https://www.bhavaniravi.com/apache-airflow/deploying-airflow-on-kubernetes"},"https://www.bhavaniravi.com/apache-airflow/deploying-airflow-on-kubernetes")),(0,l.kt)("li",{parentName:"ul"},"Airflow Dag: ",(0,l.kt)("a",{parentName:"li",href:"https://www.projectpro.io/recipes/use-sparksubmitoperator-airflow-dag"},"https://www.projectpro.io/recipes/use-sparksubmitoperator-airflow-dag"))))}w.isMDXComponent=!0},5162:(e,t,a)=>{a.d(t,{Z:()=>i});var r=a(7294),n=a(6010);const l={tabItem:"tabItem_Ymn6"};function i(e){let{children:t,hidden:a,className:i}=e;return r.createElement("div",{role:"tabpanel",className:(0,n.Z)(l.tabItem,i),hidden:a},t)}},4866:(e,t,a)=>{a.d(t,{Z:()=>N});var r=a(7462),n=a(7294),l=a(6010),i=a(2466),s=a(6550),o=a(1980),p=a(7392),u=a(12);function c(e){return function(e){return n.Children.map(e,(e=>{if(!e||(0,n.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:a,attributes:r,default:n}}=e;return{value:t,label:a,attributes:r,default:n}}))}function k(e){const{values:t,children:a}=e;return(0,n.useMemo)((()=>{const e=t??c(a);return function(e){const t=(0,p.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,a])}function d(e){let{value:t,tabValues:a}=e;return a.some((e=>e.value===t))}function m(e){let{queryString:t=!1,groupId:a}=e;const r=(0,s.k6)(),l=function(e){let{queryString:t=!1,groupId:a}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!a)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return a??null}({queryString:t,groupId:a});return[(0,o._X)(l),(0,n.useCallback)((e=>{if(!l)return;const t=new URLSearchParams(r.location.search);t.set(l,e),r.replace({...r.location,search:t.toString()})}),[l,r])]}function h(e){const{defaultValue:t,queryString:a=!1,groupId:r}=e,l=k(e),[i,s]=(0,n.useState)((()=>function(e){let{defaultValue:t,tabValues:a}=e;if(0===a.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!d({value:t,tabValues:a}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${a.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const r=a.find((e=>e.default))??a[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:t,tabValues:l}))),[o,p]=m({queryString:a,groupId:r}),[c,h]=function(e){let{groupId:t}=e;const a=function(e){return e?`docusaurus.tab.${e}`:null}(t),[r,l]=(0,u.Nk)(a);return[r,(0,n.useCallback)((e=>{a&&l.set(e)}),[a,l])]}({groupId:r}),g=(()=>{const e=o??c;return d({value:e,tabValues:l})?e:null})();(0,n.useLayoutEffect)((()=>{g&&s(g)}),[g]);return{selectedValue:i,selectValue:(0,n.useCallback)((e=>{if(!d({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);s(e),p(e),h(e)}),[p,h,l]),tabValues:l}}var g=a(2389);const b={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};function f(e){let{className:t,block:a,selectedValue:s,selectValue:o,tabValues:p}=e;const u=[],{blockElementScrollPositionUntilNextRender:c}=(0,i.o5)(),k=e=>{const t=e.currentTarget,a=u.indexOf(t),r=p[a].value;r!==s&&(c(t),o(r))},d=e=>{let t=null;switch(e.key){case"Enter":k(e);break;case"ArrowRight":{const a=u.indexOf(e.currentTarget)+1;t=u[a]??u[0];break}case"ArrowLeft":{const a=u.indexOf(e.currentTarget)-1;t=u[a]??u[u.length-1];break}}t?.focus()};return n.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,l.Z)("tabs",{"tabs--block":a},t)},p.map((e=>{let{value:t,label:a,attributes:i}=e;return n.createElement("li",(0,r.Z)({role:"tab",tabIndex:s===t?0:-1,"aria-selected":s===t,key:t,ref:e=>u.push(e),onKeyDown:d,onClick:k},i,{className:(0,l.Z)("tabs__item",b.tabItem,i?.className,{"tabs__item--active":s===t})}),a??t)})))}function v(e){let{lazy:t,children:a,selectedValue:r}=e;const l=(Array.isArray(a)?a:[a]).filter(Boolean);if(t){const e=l.find((e=>e.props.value===r));return e?(0,n.cloneElement)(e,{className:"margin-top--md"}):null}return n.createElement("div",{className:"margin-top--md"},l.map(((e,t)=>(0,n.cloneElement)(e,{key:t,hidden:e.props.value!==r}))))}function w(e){const t=h(e);return n.createElement("div",{className:(0,l.Z)("tabs-container",b.tabList)},n.createElement(f,(0,r.Z)({},e,t)),n.createElement(v,(0,r.Z)({},e,t)))}function N(e){const t=(0,g.Z)();return n.createElement(w,(0,r.Z)({key:String(t)},e))}},9850:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/skaffold-architecture-04ec12ff6f7ea3c159df0acd2f8b7cda.png"},9266:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/spark-on-k8s-41426f75a2dc0b543430a8d6c2fdcc15.png"},4746:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/spark-submit-on-k8s--using-airflow-43841e8802d08d5ee628ca6383ebd326.svg"},2647:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/spark-submit-on-k8s--using-jobs-3d715dcdd60391673bc0833f0f00d77d.svg"},2670:(e,t,a)=>{a.d(t,{Z:()=>r});const r=a.p+"assets/images/spark-submit-process-3ee98a82c76d0a6591085a837ff3b8fa.svg"}}]);