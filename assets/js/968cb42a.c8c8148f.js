"use strict";(self.webpackChunksharek_dev=self.webpackChunksharek_dev||[]).push([[1427],{3905:(e,n,t)=>{t.d(n,{Zo:()=>u,kt:()=>m});var r=t(7294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,r,i=function(e,n){if(null==e)return{};var t,r,i={},a=Object.keys(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=r.createContext({}),p=function(e){var n=r.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},u=function(e){var n=p(e.components);return r.createElement(s.Provider,{value:n},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},g=r.forwardRef((function(e,n){var t=e.components,i=e.mdxType,a=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),c=p(t),g=i,m=c["".concat(s,".").concat(g)]||c[g]||d[g]||a;return t?r.createElement(m,o(o({ref:n},u),{},{components:t})):r.createElement(m,o({ref:n},u))}));function m(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var a=t.length,o=new Array(a);o[0]=g;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[c]="string"==typeof e?e:i,o[1]=l;for(var p=2;p<a;p++)o[p]=t[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}g.displayName="MDXCreateElement"},1172:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>l,toc:()=>p});var r=t(7462),i=(t(7294),t(3905));const a={title:"Plugin Framework",description:"Explore the internals of Spark's Plugin Framework",image:"/img/plugins.jpg",keywords:["spark","internal","plugins"]},o=void 0,l={unversionedId:"spark/internals/plugin-framework",id:"spark/internals/plugin-framework",title:"Plugin Framework",description:"Explore the internals of Spark's Plugin Framework",source:"@site/sections/data_processing/spark/02-internals/plugin-framework.mdx",sourceDirName:"spark/02-internals",slug:"/spark/internals/plugin-framework",permalink:"/data_processing/spark/internals/plugin-framework",draft:!1,tags:[],version:"current",frontMatter:{title:"Plugin Framework",description:"Explore the internals of Spark's Plugin Framework",image:"/img/plugins.jpg",keywords:["spark","internal","plugins"]},sidebar:"docs",previous:{title:"Overview",permalink:"/data_processing/spark/internals/index"},next:{title:"Create a Spark plugin",permalink:"/data_processing/spark/customization/create-plugin"}},s={},p=[{value:"Spark Plugin Framework",id:"spark-plugin-framework",level:2},{value:"Features :",id:"features-",level:2},{value:"Internals :",id:"internals-",level:2},{value:"SparkPlugin Interface",id:"sparkplugin-interface",level:3},{value:"DriverPlugin Interface",id:"driverplugin-interface",level:3},{value:"ExecutorPlugin Interface",id:"executorplugin-interface",level:3},{value:"Initialization Process :",id:"initialization-process-",level:3}],u={toc:p},c="wrapper";function d(e){let{components:n,...t}=e;return(0,i.kt)(c,(0,r.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("head",null,(0,i.kt)("meta",{name:"twitter:card",content:"summary_large_image"})),(0,i.kt)("h2",{id:"spark-plugin-framework"},"Spark Plugin Framework"),(0,i.kt)("p",null,"It was introduced in ",(0,i.kt)("a",{parentName:"p",href:"https://spark.apache.org/docs/3.0.0/"},"Spark 3.0"),".\nThis plugin allows users to plug custom code to the driver or/and worker instances at start time. This allows customization to be fairly simple and straightforward. It allows to have custom metrics tracking and more control over the Spark application."),(0,i.kt)("h2",{id:"features-"},"Features :"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Access to the Spark Context"),"\nSpark plugin gives access to the Spark Context instance and thus a way in to application metrics."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Ability to Communicate Between Driver and Executor"),"\nSpark plugin framework exposes a RPC communication option between driver and executor plugins. This communication can be used to send any user defined messages between executors and driver."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Ability to push dynamic events to driver and executor"),"\nSpark plugin framework allows user to run arbitrary listeners on driver or executor side. This allows for a communication to spark JVM\u2019s from the external application. As these plugins have access to spark context, this will allow for dynamic control of the execution from outside which is very powerful.")),(0,i.kt)("h2",{id:"internals-"},"Internals :"),(0,i.kt)("p",null,"To better understand the plugin internal working, we reviewed the ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/apache/spark/pull/26170/files#diff-6505fb31dc1cffa87dd4f44d94f14820478c0d3729af8d25b4ff71fc14d5ca20"},"Plugin Framework Pull Request")," on Github."),(0,i.kt)("h3",{id:"sparkplugin-interface"},"SparkPlugin Interface"),(0,i.kt)("p",null,"This is the plugin interfaces :"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"@DeveloperApi\npublic interface SparkPlugin {\n\n  /**\n   * Return the plugin's driver-side component.\n   *\n   * @return The driver-side component, or null if one is not needed.\n   */\n  DriverPlugin driverPlugin();\n\n  /**\n   * Return the plugin's executor-side component.\n   *\n   * @return The executor-side component, or null if one is not needed.\n   */\n  ExecutorPlugin executorPlugin();\n\n}\n")),(0,i.kt)("p",null,"To create a driver plugin only, one can set the ",(0,i.kt)("inlineCode",{parentName:"p"},"executorPlugin()")," to return ",(0,i.kt)("inlineCode",{parentName:"p"},"null"),"."),(0,i.kt)("h3",{id:"driverplugin-interface"},"DriverPlugin Interface"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"// Init Method\n// This method is called at the beginning of driver initialisation. \n// It has access to spark context and plugin context. \n// The method returns a map which will be passed to executor plugin.\ndef init(SparkContext sc, PluginContext pluginContext): Map[String, String]\n\n// This method is used for the tracking custom metrics in driver side.\ndef registerMetrics(appId: String, pluginContext: PluginContext): Unit\n\n// This method is used for receiving RPC messages sent by the executors.\ndef receive(message: scala.Any): AnyRef\n\n// This method is called when driver getting shutdown.\ndef shutdown(): Unit\n")),(0,i.kt)("h3",{id:"executorplugin-interface"},"ExecutorPlugin Interface"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},"// Init Method\n// This method is called when an executor is started. extraConf are the parameters sent by driver.\ndef init(ctx: PluginContext, extraConf: util.Map[String, String]):Unit\n\n// This method is called when driver getting shutdown.\ndef shutdown(): Unit\n")),(0,i.kt)("h3",{id:"initialization-process-"},"Initialization Process :"),(0,i.kt)("p",null,"The plugins are loaded at ",(0,i.kt)("inlineCode",{parentName:"p"},"SparkContext")," initialization phase. Here is the Plugin loader (",(0,i.kt)("inlineCode",{parentName:"p"},"container"),")."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala"},'object PluginContainer {\n\n  val EXTRA_CONF_PREFIX = "spark.plugins.internal.conf."\n\n  ...\n\n  private def apply(ctx: Either[SparkContext, SparkEnv]): Option[PluginContainer] = {\n    val conf = ctx.fold(_.conf, _.conf)\n    val plugins = Utils.loadExtensions(classOf[SparkPlugin], conf.get(PLUGINS).distinct, conf)\n    if (plugins.nonEmpty) {\n      ctx match {\n        case Left(sc) => Some(new DriverPluginContainer(sc, plugins))\n        case Right(env) => Some(new ExecutorPluginContainer(env, plugins))\n      }\n    } else {\n      None\n    }\n  }\n}\n')),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"EXTRA_CONF_PREFIX")," shows that one can pass custom configurations to the plugin at submit time."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-scala",metastring:"{3}","{3}":!0},'      if (extraConf != null) {\n        extraConf.asScala.foreach { case (k, v) =>\n          sc.conf.set(s"${PluginContainer.EXTRA_CONF_PREFIX}$name.$k", v)\n        }\n      }\n')),(0,i.kt)("p",null,"Further down, we see that plugin configurations should start with the ",(0,i.kt)("inlineCode",{parentName:"p"},"EXTRA_CONF_PREFIX")," then the plugin name.     "),(0,i.kt)("admonition",{type:"note"},(0,i.kt)("p",{parentName:"admonition"},"Plugin configurations should follow this pattern:"),(0,i.kt)("p",{parentName:"admonition"},(0,i.kt)("inlineCode",{parentName:"p"},"spark.plugins.internal.conf.[PluginName].[ConfigurationName]"))),(0,i.kt)("p",null,"A ",(0,i.kt)("inlineCode",{parentName:"p"},"PluginContext")," is passed in to both driver and executor plugins. Here is it's interface :"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-java"},"public interface PluginContext {\n\n  /**\n   * Registry where to register metrics published by the plugin associated with this context.\n   */\n  MetricRegistry metricRegistry();\n\n  /** Configuration of the Spark application. */\n  SparkConf conf();\n\n  /** Executor ID of the process. On the driver, this will identify the driver. */\n  String executorID();\n\n  /** The host name which is being used by the Spark process for communication. */\n  String hostname();\n\n  /**\n   * Send a message to the plugin's driver-side component.\n   */\n  void send(Object message) throws IOException;\n\n  /**\n   * Send an RPC to the plugin's driver-side component.\n   */\n  Object ask(Object message) throws Exception;\n}\n")),(0,i.kt)("p",null,"It allows the ",(0,i.kt)("inlineCode",{parentName:"p"},"ExecutorPlugin")," to communicate with the ",(0,i.kt)("inlineCode",{parentName:"p"},"DriverPlugin")," throught RPC protocol."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Resources")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/cerndb/SparkPlugins"},"https://github.com/cerndb/SparkPlugins")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://books.japila.pl/apache-spark-internals/plugins/#resources"},"https://books.japila.pl/apache-spark-internals/plugins/#resources"))))}d.isMDXComponent=!0}}]);