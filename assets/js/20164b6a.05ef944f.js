"use strict";(self.webpackChunksharek_dev=self.webpackChunksharek_dev||[]).push([[4579],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>h});var r=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var l=r.createContext({}),p=function(e){var t=r.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},d=function(e){var t=p(e.components);return r.createElement(l.Provider,{value:t},e.children)},m="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),m=p(a),u=n,h=m["".concat(l,".").concat(u)]||m[u]||c[u]||i;return a?r.createElement(h,o(o({ref:t},d),{},{components:a})):r.createElement(h,o({ref:t},d))}));function h(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,o=new Array(i);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[m]="string"==typeof e?e:n,o[1]=s;for(var p=2;p<i;p++)o[p]=a[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}u.displayName="MDXCreateElement"},3491:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var r=a(7462),n=(a(7294),a(3905));const i={title:"Join algorithms"},o=void 0,s={unversionedId:"spark/internals/join-algorithms",id:"spark/internals/join-algorithms",title:"Join algorithms",description:"In Apache Spark, join algorithms are strategies used to physically combine datasets during a join operation. These algorithms determine how Spark organizes and processes data to perform the join efficiently. Spark provides several join algorithms to cater to different use cases and optimize performance based on the characteristics of the data being joined. Here are some of the common join algorithms in Apache Spark:",source:"@site/sections/data_processing/spark/03-internals/02-join-algorithms.md",sourceDirName:"spark/03-internals",slug:"/spark/internals/join-algorithms",permalink:"/data_processing/spark/internals/join-algorithms",draft:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"Join algorithms"},sidebar:"docs",previous:{title:"Overview",permalink:"/data_processing/spark/internals/index"},next:{title:"Query Hints",permalink:"/data_processing/spark/internals/hints"}},l={},p=[],d={toc:p},m="wrapper";function c(e){let{components:t,...a}=e;return(0,n.kt)(m,(0,r.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("p",null,"In Apache Spark, join algorithms are strategies used to physically combine datasets during a join operation. These algorithms determine how Spark organizes and processes data to perform the join efficiently. Spark provides several join algorithms to cater to different use cases and optimize performance based on the characteristics of the data being joined. Here are some of the common join algorithms in Apache Spark:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"Sort Merge Join"),":"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Description"),": Sort Merge Join is used when both input datasets are large and can be sorted based on the join keys. It involves sorting both datasets by the join keys and then merging them using a technique similar to the merge step in merge sort."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Advantages"),": Efficient for large datasets, requires minimal memory overhead, performs well when data is already sorted or can be sorted without excessive shuffling."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Limitations"),": Can be slower if data isn't sorted initially, requires both datasets to fit in memory after sorting."))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"Broadcast Hash Join"),":"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Description"),": Broadcast Hash Join is used when one of the input datasets (usually the smaller one) can fit entirely in memory. The smaller dataset is broadcasted to all worker nodes, and the larger dataset is partitioned and joined with the broadcasted data."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Advantages"),": Efficient for small tables, reduces network communication overhead, avoids shuffling for the broadcasted dataset."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Limitations"),": Limited by memory capacity for the smaller dataset, not suitable for very large datasets."))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"Shuffled Hash Join"),":"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Description"),": Shuffled Hash Join is used when both input datasets are large and cannot fit in memory. It involves partitioning both datasets based on their join keys, shuffling the partitions to the appropriate worker nodes, and then performing the join locally on each node."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Advantages"),": Scalable for large datasets, can handle skewed data distribution, suitable for cases where data sizes are not known in advance."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Limitations"),": Involves data shuffling, which can lead to high network and I/O overhead, requires careful partitioning to avoid data skew."))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"Broadcast Nested Loop Join"),":"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Description"),": Similar to the traditional nested loop join, but used when one of the input datasets is small enough to fit in memory. The small dataset is broadcasted, and the join is performed using nested loops on the larger dataset."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Advantages"),": Useful for small datasets, avoids shuffling and reduces network overhead for the broadcasted dataset."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Limitations"),": Slower for large datasets due to the nested loops, not suitable when data sizes are large and can't fit in memory."))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("strong",{parentName:"p"},"Cartesian Join"),":"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Description"),": Also known as a cross join, Cartesian Join combines every row from one dataset with every row from another dataset. It doesn't require any specific algorithms since it's a straightforward combination of rows."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Advantages"),": Simple to understand, no need for special algorithms."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Limitations"),": Generates a large number of output rows (size of dataset A * size of dataset B), can be extremely resource-intensive and slow for large datasets.")))),(0,n.kt)("p",null,"The choice of join algorithm depends on factors like the size of datasets, available memory, data distribution, and skewness of data. Spark's optimizer will automatically choose the appropriate join algorithm based on the context and available resources, but understanding these algorithms helps in designing efficient Spark applications."))}c.isMDXComponent=!0}}]);