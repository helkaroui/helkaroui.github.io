---
title: Quick wins
---

Here are simple rules to follow to avoid crashing your spark job :

## Avoid collecting to your driver
If your Dataset is so large that all of it's elements won't fit in memory on the drive machine, don't collect dataset
to the driver. Collect will attempt to copy every single element in the RDD onto the single driver program, and then run
out of memory and crash.

```scala
val values = ds.collect()
```

Instead, you can make sure the number of elements you return is capped by calling `take` or `takeSample`, or perhaps
filtering or sampling your Dataset.

```scala
val values = ds.take(10)
```

Similarly, be cautious of these other actions as well unless you are sure your dataset size is small enough to fit in
memory:
* `countByKey`
* `countByValue`
* `collectAsMap`


## Avoid collect_list


## Avoid counting
Don't use count() when you don't need to return the exact number of rows. You can use this :

```scala
if (df.takeAsList(1).size() == 0) {...}

or

if (df.queryExecution.toRdd.isEmpty()) {...}

or

if (ds.rdd.isEmpty()) {...}
```

With RDDs, you can use isEmpty() because if you see [the code](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala):
```scala
  /**
   * @note Due to complications in the internal implementation, this method will raise an
   * exception if called on an RDD of `Nothing` or `Null`. This may be come up in practice
   * because, for example, the type of `parallelize(Seq())` is `RDD[Nothing]`.
   * (`parallelize(Seq())` should be avoided anyway in favor of `parallelize(Seq[T]())`.)
   * @return true if and only if the RDD contains no elements at all. Note that an RDD
   *         may be empty even when it has at least 1 partition.
   */
  def isEmpty(): Boolean = withScope {
    partitions.length == 0 || take(1).length == 0
  }
```


##  use Broadcast for small datasets
Spark can “broadcast” a small DataFrame by sending all the data in that small DataFrame to all nodes in the cluster.
After the small DataFrame is broadcasted, Spark can perform a join without shuffling any of the data in the large DataFrame.

Let’s create a DataFrame with information about people and another DataFrame with information about cities. In this
example, the peopleDF is huge and the citiesDF is tiny. In this case broadcasting the citiesDF will accelerate the join.

```scala {7}
val citiesDF = Seq(
  ("medellin", "colombia", 2.5),
  ("bangalore", "india", 12.3)
).toDF("city", "country", "population")

peopleDF.join(
  broadcast(citiesDF),
  peopleDF("city") <=> citiesDF("city")
).show()
```

By default, spark will broadcast automatically any dataset with size under 10 MB. You can modify this threshold with
 `spark.sql.autoBroadcastJoinThreshold` property.