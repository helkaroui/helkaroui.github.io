---
title: ReduceByKey vs GroupByKey
---
import useBaseUrl from '@docusaurus/useBaseUrl';

In this article we are demystifying two known Spark Operators: `reduceByKey` and `groupByKey`

## groupByKey
Imagine a Dataset distributed in 3 partitions. The dataset consist of a transactional table with in each row the revenue
of a store located in different countries. We want to sum these revenues at country level.


```scala title="GroupByKey example"
case class Record(country: String, revenue: Double)

import spark.implicits._

val ds = Seq(
    Record("USA", 112),
    Record("USA", 250),
    Record("USA", 360),
    Record("INIDA", 102),
    Record("INDIA", 205),
).toDS[Record]

ds
  .groupbByKey(_.country)
  .mapGroups((country, revenues) => Record(country, revenues.map(_.revenue).sum))
```

a `groupByKey` will start by moving data from original partitions and create a partition per country (aggregation key).
Then, it will apply the aggregation function (here a `sum` function).

<div style={{textAlign: 'center'}}><img alt="spark reducebykey" src={useBaseUrl('/img/groupByKeyDiagram.svg')} /></div>

As shown in the diagram, the data will be repartitioned and thus a shuffle will take place. In real life data is not
naturally balanced, so in case where a key is too frequent, it's related data will be put in one partition.


## reduceByKey
A `reduceByKey` will operate in two steps: (1) In each partition it will apply the aggregation function locally
(2) all the key-value pairs are then shuffled around, send over wire, and finally being reduced to get the final result.

Here is the implementation of the same example using reduceByKey :
```scala
case class Record(country: String, revenue: Double)

import spark.implicits._

val ds = Seq(
    Record("USA", 112),
    Record("USA", 250),
    Record("USA", 360),
    Record("INIDA", 102),
    Record("INDIA", 205),
).toDS[Record]

ds
  .rdd
  .keyBey(_.country)
  .reduceByKey((r1, r2) => r1.copy(revenue = r1.revenue + r2.revenue ) )
```

Look at the diagram below to understand what happens with reduceByKey. Notice how pairs on the same machine with the
same key are combined (by using the lamdba function passed into reduceByKey) before the data is shuffled. And only the
partial sum results are sent over the wire to be reduced.

<div style={{textAlign: 'center'}}><img alt="spark reducebykey" src={useBaseUrl('/img/reduceByKeyDiagram.svg')} /></div>

This means we are more likely to have an OOM exception with a groupByKey than a reduceByKey.

:::info

### How OOM exception occurs ?
Spark spills data to disk when there is more data shuffled onto a single executor machine than can fit in memory.
However, it flushes out the data to disk one key at a time - so if a single key has more key-value pairs than can fit
in memory, an out of memory exception occurs.

:::

While both of these functions will produce the correct answer, the reduceByKey example works much better on a large
dataset. That's because Spark knows it can combine output with a common key on each partition before shuffling the data.


## When to avoid `groupByKey` ?
You can imagine that for a much larger dataset size, the difference in the amount of data you are shuffling becomes
more exaggerated and different between reduceByKey and groupByKey. Here are some rules when to avoid a `groupByKey` :

* When returning a type different from your input value type. You would like to favor a `combineByKey` in this case.
* When merging the values for each key using an associative function. You would like to use a `foldByKey` operator.


## When to avoid `reduceByKey` ?
This operator is only available with RDD API, thus switching from typed dataset to rdd and back, induces performance
reduction due to java serialization.

## Conclusion
Choosing the right operator can be tricky the first time, but can help to optimize long-running jobs quickly.