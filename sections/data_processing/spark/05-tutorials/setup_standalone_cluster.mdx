---
title: Host your own Spark cluster
---
import useBaseUrl from '@docusaurus/useBaseUrl'; 

In this article we will go through the steps of deploying a standalone [Spark](https://spark.apache.org/) cluster on a bunch of virtual machines. 
We will also run a simple application to test the setup and review the logs using Spark history server.
At the end, we will put the setup steps into an ansible playbook to have a full automation. 
All resources and produced scripts of this article can be retrieved from github repo [ansible-vagrant-stacks](https://github.com/helkaroui/ansible-vagrant-stacks/tree/main/spark).


## Design
We want to create 3 virtual machines and deploy a standalone Spark cluster on them. These VMs should have the following IP addresses:
<div style={{textAlign: 'center'}}><img alt="Spark standalone cluster" src={useBaseUrl('/img/setup_spark_cluster/architecture_diagram.png')} /></div>

For this task, we're gonna use [Vagrant](https://www.vagrantup.com/) to automatically create and start virtual machines. Then Ansible will be used to install the dependencies automatically.

## Requirements
- VirtualBox
- Vagrant
- Ansible

### 1. Setup VirtualBox
[VirtualBox](https://www.virtualbox.org/wiki/Downloads) is a powerful virtualization tool that we will use to create virtual machines. 

To setup VirtualBox, run the following commands :
```shell title=Install VirtualBox on Ubuntu
wget -q https://www.virtualbox.org/download/oracle_vbox_2016.asc -O- | sudo apt-key add -

sudo apt-add-repository "deb [arch=amd64] http://download.virtualbox.org/virtualbox/debian $(lsb_release -sc) contrib"

sudo apt-get update && sudo apt-get install -y virtualbox-6.1
```

Now you can run VirtualBox from the Ubuntu Launcher or just run `virtualbox` in your command line.

<div style={{textAlign: 'center'}}><img alt="VirtualBox fresh installation" src={useBaseUrl('/img/setup_spark_cluster/virtualbox_install.png')} /></div>

We also need to download *VM VirtualBox Extension Pack* which is a set of extensions of the Oracle VM VirtualBox base package. The extension pack provides functionalities related to connectivity and remote desktop features.
To install it, refer to the [download](https://www.virtualbox.org/wiki/Downloads) page or run the following command :

```shell title=Download VirtualBox Extension Pack
wget https://download.virtualbox.org/virtualbox/6.1.22/Oracle_VM_VirtualBox_Extension_Pack-6.1.22.vbox-extpack
```

In VirtualBox, we need to navigate the menu :  `File -> Preference -> Extensions`, then click on the add button and select the package we just downloaded.
Once there, the extension will be visible in the window

<div style={{textAlign: 'center'}}><img alt="VirtualBox Extensions" src={useBaseUrl('/img/setup_spark_cluster/virtualbox_extensions.png')} /></div>

### 2. Install Vagrant
Vagrant is a simple and powerful automation tool to deal with VMs. It uses a declarative configuration file which describes all your software requirements. To start let's install Vagrant. Go to [Download page](https://www.vagrantup.com/downloads) and choose the right download steps for your OS.

For Debian based OS :
```shell
curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -

sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"

sudo apt-get update && sudo apt-get install vagrant
```

Once installed, run `vagrant --help` to verify the installation.

### 3. Install Ansible

## Create VMs with Vagrant
**Create a Vagrant configuration**

Now let's create a project directory, in which we create a file named `Vagrantfile`.
```shell
mkdir spark-cluster && cd spark-cluster
vim Vagrantfile
```

So in the first part, we declare the base image we gonna use `ubuntu/xenial64`
```ruby title=Vagrantfile
# -*- mode: ruby -*-
# vi: set ft=ruby :

VAGRANTFILE_API_VERSION = "2"
IMAGE_NAME = "ubuntu/xenial64"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  config.vm.box = IMAGE_NAME

  config.vm.synced_folder ".", "/vagrant", disabled: true

  config.vm.provider :virtualbox do |v|
    v.memory = 1024
    v.cpus = 2
    v.linked_clone = true
  end

  boxes = [
      {:name => "master",   :ip => "10.11.13.10"},
      {:name => "worker-1", :ip => "10.11.13.11"},
      {:name => "worker-2", :ip => "10.11.13.12"}
  ]

  # Provision each of the VMs.
  boxes.each do |opts|
    config.vm.define opts[:name] do |config|
      config.vm.hostname = opts[:name]
      config.vm.network :private_network, ip: opts[:ip]
    end
  end

end
```

To start the VMs, just run :
```shell
vagrant up
```

The VMs are now up and running !
you can check there status by running `vagrant status`.
```text
Current machine states:

master                    running (virtualbox)
worker-1                  running (virtualbox)
worker-2                  running (virtualbox)
```

Also you can see the VMs on VirtualBox ui :
<div style={{textAlign: 'center'}}><img alt="Spark reduceByKey" src={useBaseUrl('/img/setup_spark_cluster/virtualbox.png')} /></div>

Now to ssh into each one on them, just run :
```title=vagrant ssh master
vagrant@master:~$ 
```

:::info

To shut down all the VMs at once, just run `vagrant halt`
To remove the VMs completely, just run `vagrant destroy`

:::

## Setup Spark Nodes
### Install Dependencies

Spark runs on Java 8/11, Scala 2.12. So we need to have Java JRE 8 installed (also JDK seems to be a pre-requisite as well). 
On Debian based OS (like Ubuntu), this is as simple as running this command :

```shell
sudo apt-get install default-jdk
```

:::info

**Spark** runs on Java 8/11, Scala 2.12, Python 3.6+ and R 3.5+. 

Java 8 prior to version 8u92 support is deprecated as of Spark 3.0.0. For the Scala API, Spark 3.1.2 uses Scala 2.12. 
You will need to use a compatible Scala version (2.12.x).

:::

Then check the java version by running :
```shell
java -version
```

The output tells you that Java is located in `/usr/lib/jvm/default-java`.


### Download Spark
Download latest Spark version when you are going to install. At writing time it's **v3.1.2** :
```
wget https://mirrors.ircam.fr/pub/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz /tmp/spark/
```

Extract the package to `/opt/spark` directory
```
tar xzvf /tmp/spark/spark-3.1.2-bin-hadoop3.2.tgz -c /opt/spark/
```

**Spark** uses environment variables to locate it's HOME_DIR and java, so we need to add those variables to `~/.bashrc`
```shell title="Edit .bashrc file"
sudo vim ~/.bashrc
```

And add the following entries at the end :
```shell
export JAVA_HOME=/usr/lib/jvm/default-java
export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
```

Now source the file to apply those changes to the current session :
```
source ~/.bashrc
```

Latest Apache Spark is successfully installed in your VM !

### Verify installation
To verify the installation, close the Terminal already opened, and open a new Terminal again. Run the following command :
```
spark-shell
```

### Configure the Master node
The most important configuration files you need to know about are :
- `conf/spark-defaults.conf` : control most application parameters
- `conf/spark-env.sh` : can be used to set per-machine settings, such as the IP address
- `conf/log4j.properties` : can be used to configure applications logging properties.

Let's configure each one of them :

Edit `spark-defaults.conf` and put the following properties inside :
```shell
vim /opt/spark/conf/spark-defaults.conf
``` 

```text
spark.master            spark://10.11.13.10:7077
spark.executor.memory   1g
spark.eventLog.enabled  true
spark.serializer        org.apache.spark.serializer.KryoSerializer
```


Copy and edit `spark-env.sh` file :
```shell
cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh
vim $SPARK_HOME/conf/spark-env.sh
```

Add the following line :
```text
export SPARK_MASTER_HOST=10.11.13.10
```

Copy and edit `slaves` file :
```shell
cp $SPARK_HOME/conf/slaves.template $SPARK_HOME/conf/slaves
vim $SPARK_HOME/conf/slaves
```

Add IP address of slaves. Delete "localhost" #if present
```text
10.11.13.11
10.11.13.12
```

## Run the cluster
From spark master node, run the following command :
```shell
$SPARK_HOME/sbin/start-all.sh
```

The spark ui will be available on [10.11.13.10:8080](http://10.11.13.10:8080)

## Submitting Application to Cluster
We can check if our cluster is correctly running by submitting an application. Spark comes with some examples, run the code below to calculate the value of Pi.

```shell
spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://10.11.13.10:7077 \
  $SPARK_HOME/examples/jars/spark-examples_2.12-3.1.2.jar
```
In the console, you will see the following output : 

```text title="Compute PI" {11}
21/06/20 10:10:49 INFO SparkContext: Running Spark version 3.1.2
21/06/20 10:10:49 INFO ResourceUtils: ==============================================================
21/06/20 10:10:49 INFO ResourceUtils: No custom resources configured for spark.driver.
21/06/20 10:10:49 INFO ResourceUtils: ==============================================================
21/06/20 10:10:50 INFO SparkContext: Submitted application: Spark Pi
...
21/06/20 10:10:57 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 3.687 s
21/06/20 10:10:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/06/20 10:10:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/06/20 10:10:57 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 3.807045 s
Pi is roughly 3.141715708578543
21/06/20 10:10:57 INFO SparkUI: Stopped Spark web UI at http://192.168.1.71:4040
21/06/20 10:10:57 INFO StandaloneSchedulerBackend: Shutting down all executors
```

Once completed, the application name will show up in the "Completed Applications" section in the Spark UI:
<div style={{textAlign: 'center'}}><img alt="Spark UI" src={useBaseUrl('/img/setup_spark_cluster/pi_application.png')} /></div>

## Using Spark History Server
To check applications logs, you can run Spark history server on master node :
```shell
start-history-server.sh
```

The UI will be available on [10.11.13.10:18080](http://10.11.13.10:18080)
<div style={{textAlign: 'center'}}><img alt="Spark History Server UI" src={useBaseUrl('/img/setup_spark_cluster/spark_history_server.png')} /></div>


## "One Playbook to unite them all"
Finally, we don't want to repeat all these steps each time we add a new worker node. 
So what we did is, we created an Ansible playbook that provisions the VMs automatically. The project is found here : [ansible-vagrant-stacks](https://github.com/helkaroui/ansible-vagrant-stacks/tree/main/spark).

## Conclusion
In this post, we have learnt how to setup our own Spark Stand Alone Cluster and how to submit a simple application to it.
This setup will allow us to experiment with Spark's fundamental and deep dive into its internals.


