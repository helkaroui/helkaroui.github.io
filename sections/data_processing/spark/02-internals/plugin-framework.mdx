---
title: Plugin Framework
description: Explore the internals of Spark's Plugin Framework
image: /img/plugins.jpg
keywords: [spark, internal, plugins]
---

<head>
  <meta name="twitter:card" content="summary_large_image">
</head>

## Spark Plugin Framework
It was introduced in Spark 3.0. 
This plugin allows users to plug custom code to the driver or/and worker instances at start time. This allows customization to be fairly simple and straightforward. It allows to have custom metrics tracking and more control over the Spark application.


## Features :
**Access to the Spark Context** 
Spark plugin gives access to the Spark Context instance and thus a way in to application metrics.
**Ability to Communicate Between Driver and Executor** 
Spark plugin framework exposes a RPC communication option between driver and executor plugins. This communication can be used to send any user defined messages between executors and driver.
**Ability to push dynamic events to driver and executor** 
Spark plugin framework allows user to run arbitrary listeners on driver or executor side. This allows for a communication to spark JVMâ€™s from the external application. As these plugins have access to spark context, this will allow for dynamic control of the execution from outside which is very powerful.


## Internals :
To better understand the plugin internal working, we reviewed the [Plugin Framework Pull Request](https://github.com/apache/spark/pull/26170/files#diff-6505fb31dc1cffa87dd4f44d94f14820478c0d3729af8d25b4ff71fc14d5ca20) on Github.

### SparkPlugin Interface
This is the plugin interfaces :

```scala
@DeveloperApi
public interface SparkPlugin {

  /**
   * Return the plugin's driver-side component.
   *
   * @return The driver-side component, or null if one is not needed.
   */
  DriverPlugin driverPlugin();

  /**
   * Return the plugin's executor-side component.
   *
   * @return The executor-side component, or null if one is not needed.
   */
  ExecutorPlugin executorPlugin();

}
```
To create a driver plugin only, one can set the `executorPlugin()` to return `null`.


### DriverPlugin Interface
```scala
// Init Method
// This method is called at the beginning of driver initialisation. 
// It has access to spark context and plugin context. 
// The method returns a map which will be passed to executor plugin.
def init(SparkContext sc, PluginContext pluginContext): Map[String, String]

// This method is used for the tracking custom metrics in driver side.
def registerMetrics(appId: String, pluginContext: PluginContext): Unit

// This method is used for receiving RPC messages sent by the executors.
def receive(message: scala.Any): AnyRef

// This method is called when driver getting shutdown.
def shutdown(): Unit
```

### ExecutorPlugin Interface
```scala
// Init Method
// This method is called when an executor is started. extraConf are the parameters sent by driver.
def init(ctx: PluginContext, extraConf: util.Map[String, String]):Unit

// This method is called when driver getting shutdown.
def shutdown(): Unit
```

### Initialization Process :
The plugins are loaded at `SparkContext` initialization phase. Here is the Plugin loader (`container`).

```scala
object PluginContainer {

  val EXTRA_CONF_PREFIX = "spark.plugins.internal.conf."

  ...

  private def apply(ctx: Either[SparkContext, SparkEnv]): Option[PluginContainer] = {
    val conf = ctx.fold(_.conf, _.conf)
    val plugins = Utils.loadExtensions(classOf[SparkPlugin], conf.get(PLUGINS).distinct, conf)
    if (plugins.nonEmpty) {
      ctx match {
        case Left(sc) => Some(new DriverPluginContainer(sc, plugins))
        case Right(env) => Some(new ExecutorPluginContainer(env, plugins))
      }
    } else {
      None
    }
  }
}
```

The `EXTRA_CONF_PREFIX` shows that one can pass custom configurations to the plugin at submit time.

```scala {3}
      if (extraConf != null) {
        extraConf.asScala.foreach { case (k, v) =>
          sc.conf.set(s"${PluginContainer.EXTRA_CONF_PREFIX}$name.$k", v)
        }
      }
```

Further down, we see that plugin configurations should start with the `EXTRA_CONF_PREFIX` then the plugin name.     

:::note

Plugin configurations should follow this pattern:

`spark.plugins.internal.conf.[PluginName].[ConfigurationName]`

:::



A `PluginContext` is passed in to both driver and executor plugins. Here is it's interface :

```java
public interface PluginContext {

  /**
   * Registry where to register metrics published by the plugin associated with this context.
   */
  MetricRegistry metricRegistry();

  /** Configuration of the Spark application. */
  SparkConf conf();

  /** Executor ID of the process. On the driver, this will identify the driver. */
  String executorID();

  /** The host name which is being used by the Spark process for communication. */
  String hostname();

  /**
   * Send a message to the plugin's driver-side component.
   */
  void send(Object message) throws IOException;

  /**
   * Send an RPC to the plugin's driver-side component.
   */
  Object ask(Object message) throws Exception;
}
```

It allows the `ExecutorPlugin` to communicate with the `DriverPlugin` throught RPC protocol.


**Resources**
- https://github.com/cerndb/SparkPlugins
- https://books.japila.pl/apache-spark-internals/plugins/#resources
