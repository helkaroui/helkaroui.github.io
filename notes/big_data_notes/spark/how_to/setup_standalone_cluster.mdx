---
title: How to setup a spark standalone cluster
---
import useBaseUrl from '@docusaurus/useBaseUrl';

In this article we will go through the steps of deploying a standalone spark cluster on a VM stack. All the below steps are converted into an ansible playbook that you can find in my github repo [ansible-vagrant-stacks](https://github.com/helkaroui/ansible-vagrant-stacks/tree/main/spark).


We want to create 3 virtual machines and deploy a standalone spark cluster on them. These VMs should have the following IP addresses:
- master:   10.11.13.10
- worker-1: 10.11.13.11
- worker-2: 10.11.13.12

For this task, we're gonna use [Vagrant](https://www.vagrantup.com/) to automatically create and start virtual machines. Then Ansible will be used to install the dependencies.


## Requirements
- Install VirtualBox
- Install Vagrant
- Install Ansible

### 1. Install VirtualBox
[VirtualBox](https://www.virtualbox.org/wiki/Downloads) is a powerful virtualization tool that we will use to create virtual machines. 
In this article we will install it on a debian-based OS. Thus you need to check out the installation guide if you use a different operating system.

```shell title=Install VirtualBox on Ubuntu
wget -q https://www.virtualbox.org/download/oracle_vbox_2016.asc -O- | sudo apt-key add -

sudo apt-add-repository "deb [arch=amd64] http://download.virtualbox.org/virtualbox/debian $(lsb_release -sc) contrib"

sudo apt-get update && sudo apt-get install -y virtualbox-6.1
```

Now you can run VirtualBox from the Ubuntu Launcher or just run `virtualbox` in your command line.

<div style={{textAlign: 'center'}}><img alt="Virtualbox fresh installation" src={useBaseUrl('/img/setup_spark_cluster/virtualbox_install.png')} /></div>

We also need to download *VM VirtualBox Extension Pack* which is a set of extensions of the Oracle VM VirtualBox base package. The extension pack provides functionalities related to connectivity and remote desktop features.
To install it, refer to the [download](https://www.virtualbox.org/wiki/Downloads) page or run the following command :

```shell title=Download VirtualBox Extension Pack
wget https://download.virtualbox.org/virtualbox/6.1.22/Oracle_VM_VirtualBox_Extension_Pack-6.1.22.vbox-extpack
```

In VirtualBox, we need to navigate the menu :  `File -> Preference -> Extensions`, then click on the add button and select the package we just downloaded.
Once there, the extension will be visible in the window

<div style={{textAlign: 'center'}}><img alt="Virtualbox fresh installation" src={useBaseUrl('/img/setup_spark_cluster/virtualbox_extensions.png')} /></div>

### 2. Install Vagrant
Vagrant is a simple and powerful automation tool to deal with VMs. It uses a declarative configuration file which describes all your software requirements. To start let's install Vagrant. Go to [Download page](https://www.vagrantup.com/downloads) and choose the right download steps for your OS.

For Debian based OS :
```shell
curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -

sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"

sudo apt-get update && sudo apt-get install vagrant
```

Once installed, run `vagrant --help` to verify the installation.

### 3. Install Ansible

## Create some VMs with Vagrant
**Create a Vagrant configuration**
Now let's create a project directory, inwhich we create a file named `Vagrantfile`.
```shell
mkdir spark-cluster && cd spark-cluster
vim Vagrantfile
```

So in the first part, we declare the base image we gonna use `ubuntu/xenial64`
```ruby
# -*- mode: ruby -*-
# vi: set ft=ruby :

VAGRANTFILE_API_VERSION = "2"
IMAGE_NAME = "ubuntu/xenial64"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  config.vm.box = IMAGE_NAME

  config.vm.synced_folder ".", "/vagrant", disabled: true

  config.vm.provider :virtualbox do |v|
    v.memory = 1024
    v.cpus = 2
    v.linked_clone = true
  end

  boxes = [
      {:name => "master",   :ip => "10.11.13.10"},
      {:name => "worker-1", :ip => "10.11.13.11"},
      {:name => "worker-2", :ip => "10.11.13.12"}
  ]

  # Provision each of the VMs.
  boxes.each do |opts|
    config.vm.define opts[:name] do |config|
      config.vm.hostname = opts[:name]
      config.vm.network :private_network, ip: opts[:ip]
    end
  end

end
```

To start the VMs, just run :
```
vagrant up
```

The VMs are now up and running !
you can check there status by running `vagrant status`.
```text
Current machine states:

master                    running (virtualbox)
worker-1                  running (virtualbox)
worker-2                  running (virtualbox)
```

Also you can see the VMs on VirtualBox ui :
<div style={{textAlign: 'center'}}><img alt="spark reducebykey" src={useBaseUrl('/img/setup_spark_cluster/virtualbox.png')} /></div>

Now to ssh into each one on them, just run :
```title=vagrant ssh master
vagrant@master:~$ 
```

:::info

To shut down all the VMs at once, just run `vagrant halt`
To remove the VMs completely, just run `vagrant destroy`

:::

## Setup Spark Master Node
**Install Dependencies**
Spark runs on the JVM, we need to have Java JRE 8 installed (also JDK seems to be a pre-requisite as well). On Debian based OS, this is as simple as running this command :

```shell
sudo apt-get install default-jdk
```

### Download Spark
Download latest Spark version when you are going to install. At wrinting time it's **v3.1.2** :
```
wget https://mirrors.ircam.fr/pub/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz /tmp/spark/
```

```
tar xzvf /tmp/spark/spark-3.1.2-bin-hadoop3.2.tgz -c /opt/spark/
```

```
sudo vim ~/.bashrc
```

```
export JAVA_HOME=/usr/lib/jvm/default-java/jre
export SPARK_HOME=/opt/spark/bin
export PATH=$PATH:SPARK_HOME
```

```
source ~/.bashrc
```

Latest Apache Spark is successfully installed in your VM !

### Verify installation
To verify the installation, close the Terminal already opened, and open a new Terminal again. Run the following command :
```
spark-shell
```

### Configure the Master node
The most important configuration files you need to know about are :
- `conf/spark-defaults.conf` : control most application parameters
- `conf/spark-env.sh` : can be used to set per-machine settings, such as the IP address
- `conf/log4j.properties` : can be used to configure applications logging properties.

Let's configure each one of them :

Edit `spark-defaults.conf` and put the following properties inside :
```
vim /opt/spark/conf/spark-defaults.conf
``` 

```
spark.master            spark://5.6.7.8:7077
spark.executor.memory   1g
spark.eventLog.enabled  true
spark.serializer        org.apache.spark.serializer.KryoSerializer
```


Copy and edit `spark-env.sh` file :
```
cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh
vim $SPARK_HOME/conf/spark-env.sh
```
Add the following line :
```
export SPARK_MASTER_HOST=192.168.1.71
```

Copy and edit `slaves` file :
```
cp $SPARK_HOME/conf/slaves.template $SPARK_HOME/conf/slaves
vim $SPARK_HOME/conf/slaves
```

Add IP address of slaves. Delete "localhost" #if present
```
192.168.1.72
192.168.1.71
```

## Setup Spark Worker Node
$SPARK_HOME/sbin/start-all.sh



## Submitting Application to Cluster
We can check if our cluster if functioning by submitting an application. A Spark Application detects SparkContext instance which holds the SparkConf object which specifies whether the application has to run in Local processes or Cluster processes. We will cover this in future posts, but for now just run the code below to calculate the value of Pi.

#IP=MASTER node IP address
#PORT=7077 for me
#You can check your master address by opening #https://127.0.0.1:8080/
$ MASTER=spark://IP:PORT $SPARK_HOME/bin/run-example org.apache.spark.examples.SparkPi#You should get a value and bunch of warnings


You should see the similar screen on Spark UI as shown below.



## Conclusion

In this post, we have learnt how to setup your own cluster environment in Apache Spark and how to submit a simple application to it and navigate the states of nodes using Spark UI.





Next :
- Setup Spark metrics
- 